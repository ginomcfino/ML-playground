{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 5 - Reinforcement Learning and Transformers \n",
    "\n",
    "<h4> Reinforcement Learning Section by Sid Mysore. <br> Transformers Section adapted from previous homework designed by Ruizhao Zhu with help of Brian Kulis and Ashok Cutkosky<br> </h4>\n",
    "\n",
    "---\n",
    "\n",
    "This assignment is broken up into 2 parts:\n",
    "1. Implementing and testing basic deep RL algorithms for:\n",
    "    * Discrete action spaces with DQN (simplified from the seminal Nature paper)\n",
    "    * Continuous action spaces with DDPG (a popular basic actor-critic algorithm)\n",
    "2. Understanding the basic structure of Transformers\n",
    "\n",
    "This code has been tested locally on Linux, Windows 10 and MacOS, and on Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes before running code\n",
    "\n",
    "1. If running on Windows, note that some packages may throw warnings or errors - if you encounter this, try the fixes recommended by the error message(s)\n",
    "\n",
    "2. Additional packages may be required for running locally. This notebook is configured to help you install specific versions of the required packages but if you are concerned about them overwriting exisiting configs for other projects, it is recommended to create a new python environment.\n",
    "\n",
    "2. When running code, make sure the following files are in your current working directory as they will be needed for some of the RL logging and plotting:\n",
    "    * `test_policy.py`\n",
    "    * `logx.py`\n",
    "    * `plot.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Deep Q Learning (30 Points)\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/rl_diagram.png\" width=\"360em\">\n",
    "\n",
    "The main components of the RL optimization loop are the agent and the environment. The environment is the world that the agent interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the world, and then decides on an action to take. The environment may change as a response to the agents' actions on it, but may also change on its own.\n",
    "\n",
    "The agent also perceives a reward signal from the environment, a number that tells it how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward. Reinforcement learning methods are ways that the agent can learn behaviors to achieve its goal.\n",
    "\n",
    "##### Fully vs. Partially Observable State-spaces\n",
    "\n",
    "When the agent is able to observe the complete state of the environment, we say that the environment is fully observed. When the agent can only see a partial observation, we say that the environment is partially observed. For the purposes of this homework, we will be dealing with fully observable environments.\n",
    "\n",
    "##### Discrete vs. Continuous Action-spaces\n",
    "\n",
    "Different environments allow different kinds of actions. The set of all valid actions in a given environment is often called the action space. Some environments, like old Atari games and Go, have discrete action spaces, where only a finite number of moves are available to the agent. Other environments, like where the agent controls a robot in a physical world, have continuous action spaces. In continuous spaces, actions are real-valued vectors. In this homework, we will consider problems involving both types of action spaces and some simple algorithms for solving them.\n",
    "\n",
    "##### The RL Problem\n",
    "\n",
    "The reward function R is critically important in reinforcement learning. It depends on the current state of the world, the action just taken, and the next state of the world:\n",
    "\n",
    "$r_t = R(s_t, a_t, s_{t+1})$\n",
    "\n",
    "although frequently this is simplified to just a dependence on the current state, $r_t = R(s_t)$, or state-action pair $r_t = R(s_t,a_t)$.\n",
    "\n",
    "The goal of the agent is to maximize the expected cumulative reward over a trajectory, $\\tau$, but this actually can mean a few things. Weâ€™ll notate all of these cases with $R(\\tau)$. The expected return, denoted by $J(\\pi)$, is then:\n",
    "\n",
    "$J(\\pi) = \\mathbb{E}_{\\tau\\sim \\pi}{R(\\tau)}$\n",
    "\n",
    "The central optimization problem in RL can then be expressed by\n",
    "\n",
    "$\\pi^* = \\arg \\max_{\\pi} J(\\pi)$\n",
    "\n",
    "with $\\pi^*$ being the optimal policy.\n",
    "\n",
    "##### The need to approximate value functions\n",
    "\n",
    "A way to frame the RL problem would be to say that we want actions taken by our agents to maximize the expected achievable value, given a starting state. This can be in the form of a state-value function:\n",
    "\n",
    "$V^{\\pi}(s) = \\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau) | s_0 = s]$ for states $s \\in S$\n",
    "\n",
    "or a state-action value function (often called a Q-function):\n",
    "\n",
    "$Q^{\\pi}(s,a) = \\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau) | s_0 = s, a_0 = a]$ for states $s \\in S$ and actions $a \\in A$\n",
    "\n",
    "Both value representations obey self-consistency equations called Bellman equations, of which the basic idea is that:\n",
    "\n",
    "    The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.\n",
    "\n",
    "For a given policy, $\\pi$, the Bellman equations are:\n",
    "\n",
    "$ V^{\\pi}(s_t) = \\mathbb{E}_{a \\sim \\pi}[r(s_t,a_t) + \\gamma V^\\pi(s_{t+1})]$\n",
    "\n",
    "$ Q^{\\pi}(s_t, a_t) = \\mathbb{E}_{s}[r(s_t,a_t) + \\gamma Q^\\pi(s_{t+1},\\pi(s_{t+1}))]]$\n",
    "\n",
    "The optimal policy is one that maximizes the expected values $V$ and/or $Q$. This homework will mainly focus on Q-learning techniques, so we are mainly concerned with techniques that consider the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Setup\n",
    "\n",
    "Now for some setup to use during the RL parts of this homework.\n",
    "\n",
    "We will be representing policy and value functions by neural networks. The problems we're working on are fairly simple so we'll be using some simple multi-layer perceptrons (MLPs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below if you're starting from a clean environment or suspect you may have the wrong versions of packages \n",
    "\n",
    "*(Also assumes the packages `ipython` and `jupyter` are already installed since you need them anyway to use this notebook)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[classic_control]==0.17.3 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from gym[classic_control]==0.17.3) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from gym[classic_control]==0.17.3) (1.21.5)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from gym[classic_control]==0.17.3) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from gym[classic_control]==0.17.3) (1.6.0)\n",
      "Requirement already satisfied: future in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[classic_control]==0.17.3) (0.18.2)\n",
      "Requirement already satisfied: torch==1.10.0 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (1.10.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from torch==1.10.0) (4.1.1)\n",
      "Requirement already satisfied: numpy in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (1.21.5)\n",
      "Requirement already satisfied: scipy in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (1.7.3)\n",
      "Requirement already satisfied: matplotlib in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (3.5.1)\n",
      "Requirement already satisfied: pandas in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: sympy in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (1.10.1)\n",
      "Requirement already satisfied: nose in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (1.3.7)\n",
      "Requirement already satisfied: seaborn in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (0.11.2)\n",
      "Requirement already satisfied: joblib in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from matplotlib) (9.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from matplotlib) (4.31.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from sympy) (1.2.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ginomcfino/Github/ML-playground/venv/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"gym[classic_control]==0.17.3\"\n",
    "!pip install \"torch==1.10.0\"\n",
    "!pip install numpy scipy matplotlib pandas sympy nose seaborn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from logx import EpochLogger\n",
    "from test_policy import load_policy_and_env, run_policy\n",
    "from plot import plot_data, get_datasets\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use replay buffers to store trajectories during rollouts so that we can sample from them to aid in training. By storing experiences in replay buffers and sampling from them, we can mitigate catastrophic forgetting (when RL agents forget old experiences).\n",
    "\n",
    "The Replay Buffer mainly keeps a store of the core elements of the Markov Decision Process (MDP) tuple for each interaction of the agent in the training environment. This includes the observed state (obs), the action taken (act), the reward received (rew), the resultant next state after the action (next_obs) and whether the episode was completed/terminated (done).\n",
    "\n",
    "This data can then be sampled during optimization to use for batch updates, as we will do in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, size, discrete=False):\n",
    "        self.discrete=discrete\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     done=self.done_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.int64 if (k=='act' and self.discrete) else torch.float32) for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Deep Q Networks (DQNs)\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function $Q^*:StateÃ—Action \\rightarrow \\mathbb{R}$, that could tell us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards:\n",
    "\n",
    "$\\pi^*(s_t) = \\argmax_a Q^*(s_t,a)$\n",
    "\n",
    "However, we donâ€™t know everything about the world, so we donâ€™t have access to $Q^*$. But, since neural networks are universal function approximators, we can simply create one and train it to resemble $Q^*$.\n",
    "\n",
    "For a discrete action space, a Q-network can be used to estimate the Q-values for each possible action, given a state. The policy can then be designed such that:\n",
    "\n",
    "$a^\\pi = \\argmax_a Q^\\pi(s_t,a)$\n",
    "\n",
    "For our training update rule, weâ€™ll use a fact that every $Q$ function for some policy obeys the Bellman equation:\n",
    "\n",
    "$Q^\\pi(s_t,a) = r + \\gamma Q^\\pi(s_{t+1},\\pi(s_{t+1}))$\n",
    "\n",
    "The difference between the two sides of the equality is known as the temporal difference error\n",
    "\n",
    "$\\delta = Q(s_{t},a) - (r + \\gamma \\max_a Q(s_{t+1}, a))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_space, act_space, hidden_sizes=(64,64), activation=nn.ReLU, output_activation=nn.Identity):\n",
    "        super().__init__()\n",
    "        h_sizes = [obs_space.shape[0]] + list(hidden_sizes) + [act_space.n]\n",
    "        self.Q = mlp(h_sizes, activation, output_activation)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.Q(obs)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.Q(obs[None,]).max(1)[1].numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the above simple DQN structure to train a policy to solve OpenAI gym's CartPole problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "steps_per_epoch=4000\n",
    "epochs=20\n",
    "gamma=0.99\n",
    "polyak=0.99  \n",
    "q_lr=1e-3\n",
    "batch_size=100\n",
    "start_steps=10000\n",
    "update_after=1000 \n",
    "update_every=50\n",
    "act_noise=0.1\n",
    "num_test_episodes=10\n",
    "max_ep_len=1000\n",
    "save_freq=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a location for the trained models and logs to get saved. By default, it'll be stored in the current working directory but you may change this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory location to save dqn logs and models\n",
    "dqn_output_dir = 'dqn'\n",
    "\n",
    "# Logger setup\n",
    "logger_kwargs={'output_dir':dqn_output_dir, 'exp_name':'dqn_CartPole'}\n",
    "logger = EpochLogger(**logger_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we'll be working on for DQN is the [CartPole](https://gym.openai.com/envs/CartPole-v1/) OpenAI Gym task. \n",
    "\n",
    "The agent observes the positions and velocities of the cart and pole and can take one of two discrete options to either move the cart to the left, or the right, to try and keep the pole balanced.\n",
    "\n",
    "The episode keeps going for up to 200 steps so long as the pole doesn't fall over too much, and the agent fails if the pole falls more than $\\pm 12$ degrees from the vertical. \n",
    "\n",
    "For every step that it remains 'alive', the agent gets 1 point of reward. The objective is to maximize this reward, which is equivalent to keep the pole balanced for as long as possible.\n",
    "\n",
    "Additional details for the implementation of this environment can be found via the code [source](https://github.com/openai/gym/blob/4ede9280f9c477f1ca09929d10cdc1e1ba1129f1/gym/envs/classic_control/cartpole.py).\n",
    "\n",
    "A random agent would quickly fail to balance the pole, as below (the frame hitches are due to the agent failing and the episode being reset)\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/cartpole_random_demo.gif\" width=\"360em\">\n",
    "\n",
    "But a trained agent can successfully solve this problem (using solution code for this HW).\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/cartpole_demo.gif\" width=\"360em\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Definition\n",
    "env_fn = lambda :gym.make('CartPole-v0')\n",
    "env, test_env = env_fn(), env_fn()\n",
    "obs_dim = env.observation_space.shape\n",
    "act_dim = env.action_space.shape\n",
    "\n",
    "# Seeding\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "test_env.seed(seed)\n",
    "\n",
    "# Experience buffer\n",
    "replay_size=int(1e6) \n",
    "replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size, discrete=True)\n",
    "\n",
    "# Create DQN module\n",
    "Qnet = DQN(env.observation_space, env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stabilize training we also use a 'target' Q-network which is held relatively constant and is periodically updated with weights from the DQN being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target network\n",
    "Qtarg = deepcopy(Qnet)\n",
    "\n",
    "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "for p in Qtarg.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Set up model saving\n",
    "logger.setup_pytorch_saver(Qnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can next initialize some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def get_action(o, eps_thresh):\n",
    "    a = Qnet.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "    if np.random.rand() < eps_thresh:\n",
    "        a = env.action_space.sample()\n",
    "    return a\n",
    "\n",
    "def test_agent():\n",
    "    for j in range(num_test_episodes):\n",
    "        o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "        while not(d or (ep_len == max_ep_len)):\n",
    "            # Take deterministic actions at test time (noise_scale=0)\n",
    "            o, r, d, _ = test_env.step(get_action(o, 0))\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "        logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DQN Q-learning Loss [10 points]\n",
    "\n",
    "For a sampled batch of data from the replay buffer consisting of sets of observations (o), corresponding actions taken (a), achieved reward (r), resultant observations (o2), and a flag indicating if the recorded step solved the problem or ended the episode (d), we compute the DQN loss in 4 main steps:\n",
    "\n",
    "1. Compute the Q-network output for o and extract Q-values for specific actions taken, a: $Q(s,a)$ - Actions, $a \\in [0,n-1]$, are indices into a discrete action space with $n$ possible actions.\n",
    "2. Next, compute the Bellman backup: $b = r + \\gamma \\max_a Q_{targ}(s,a)$ - remember that $\\gamma = 0$ if $d=1$ and that you don't need to track gradients through the target Q-network.\n",
    "3. Compute the Bellman error: $e = Q(s,a) - b$\n",
    "4. Finally compute the Q-loss: $l_Q = \\frac{1}{N} || e ||_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing DQN Q-loss\n",
    "def compute_loss_q(data):\n",
    "    # Batch data sample:\n",
    "    o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
    "    \n",
    "    \"\"\" STUDENT CODE GOES HERE \"\"\"\n",
    "    q = None # Compute q values for observations as appropriate\n",
    "    loss_q = None\n",
    "    \"\"\" STUDENT CODE ENDS \"\"\"\n",
    "\n",
    "    # Useful info for logging\n",
    "    loss_info = dict(QVals=q.detach().numpy())\n",
    "\n",
    "    return loss_q, loss_info\n",
    "\n",
    "# Set up optimizers for policy and q-function\n",
    "q_optimizer = Adam(Qnet.Q.parameters(), lr=q_lr)\n",
    "\n",
    "def update(data):\n",
    "    # First run one gradient descent step for Q.\n",
    "    q_optimizer.zero_grad()\n",
    "    loss_q, loss_info = compute_loss_q(data)\n",
    "    loss_q.backward()\n",
    "    q_optimizer.step()\n",
    "\n",
    "    # Record things\n",
    "    logger.store(LossQ=loss_q.item(), **loss_info)\n",
    "\n",
    "    # Finally, update target networks by polyak averaging.\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(Qnet.parameters(), Qtarg.parameters()):\n",
    "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "            p_targ.data.mul_(polyak)\n",
    "            p_targ.data.add_((1 - polyak) * p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run training.\n",
    "\n",
    "While running the code, pay attention the following logged information to determine if things are progressing well:\n",
    "\n",
    "1. For the first few thousand steps, the agents act randomly so you'll likely see low rewards that don't improve until more than `start_steps` steps have elapsed.\n",
    "2. Average Episode and Test Episode returns should generally be increasing (up to 200 which is the max for the CartPole problem)\n",
    "3. Despite optimizing the Q-value loss, the fact that we're having the network learn an arbitrary value means that we may never hit a loss of 0 - the loss should stabilize after some time though (feel free to experiment with more epochs of training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for interaction with environment\n",
    "total_steps = steps_per_epoch * epochs\n",
    "o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "for t in range(total_steps):\n",
    "    \n",
    "    # Until start_steps have elapsed, randomly sample actions\n",
    "    # from a uniform distribution for better exploration. Afterwards, \n",
    "    # use the learned policy (with some noise, via act_noise). \n",
    "    if t > start_steps:\n",
    "        a = get_action(o, act_noise)\n",
    "    else:\n",
    "        a = env.action_space.sample()\n",
    "\n",
    "    # Step the env\n",
    "    o2, r, d, _ = env.step(a)\n",
    "    ep_ret += r\n",
    "    ep_len += 1\n",
    "\n",
    "    # Ignore the \"done\" signal if it comes from hitting the time\n",
    "    # horizon (that is, when it's an artificial terminal signal\n",
    "    # that isn't based on the agent's state)\n",
    "    d = False if ep_len==max_ep_len else d\n",
    "\n",
    "    # Store experience to replay buffer\n",
    "    replay_buffer.store(o, a, r, o2, d)\n",
    "\n",
    "    # Super critical, easy to overlook step: make sure to update \n",
    "    # most recent observation!\n",
    "    o = o2\n",
    "\n",
    "    # End of trajectory handling\n",
    "    if d or (ep_len == max_ep_len):\n",
    "        logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Update handling\n",
    "    if t >= update_after and t % update_every == 0:\n",
    "        for _ in range(update_every):\n",
    "            batch = replay_buffer.sample_batch(batch_size)\n",
    "            update(data=batch)\n",
    "\n",
    "    # End of epoch handling\n",
    "    if (t+1) % steps_per_epoch == 0:\n",
    "        epoch = (t+1) // steps_per_epoch\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs):\n",
    "            logger.save_state({'env': env}, None)\n",
    "\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        test_agent()\n",
    "\n",
    "        # Log info about epoch\n",
    "        logger.log_tabular('Epoch', epoch)\n",
    "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('EpLen', average_only=True)\n",
    "        logger.log_tabular('TestEpLen', average_only=True)\n",
    "        logger.log_tabular('TotalEnvInteracts', t)\n",
    "        logger.log_tabular('QVals', with_min_and_max=True)\n",
    "        logger.log_tabular('LossQ', average_only=True)\n",
    "        logger.dump_tabular()\n",
    "\n",
    "env.close()\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "data = get_datasets(dqn_output_dir)\n",
    "plot_data(data, xaxis='TotalEnvInteracts', value='Performance', smooth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the trained agents as below (set render=True to visualize - note that additional settings may be required to view on Colab or on a remote server)\n",
    "\n",
    "Note: In order for this code to work, the DQN model definition should be in scope.\n",
    "\n",
    "The solved CartPole controller would typically have EpRet close to 200 (where 200 is the max) - If it doesn't quite plateau there, try rerunning the training (we're only training it for a short while with only sligtly tuned hyperparameters so it's possible that it may not stably plateau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_env, get_action = load_policy_and_env(dqn_output_dir, 'last', True)\n",
    "run_policy(playback_env, get_action, num_episodes=10, render=False)\n",
    "playback_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Deep Deterministic Policy Gradient (DDPG) - An Actor-Critic Algorithm\n",
    "\n",
    "Next, we'll implement a Q-learning algorithm for continuous control using a popular actor-critic algortihm, DDPG.\n",
    "\n",
    "One of the main traits of actor-critic algortihsm is that unlike with DQNs:\n",
    "1. The policy network and value estimation network are separated into two separate, distinct functions\n",
    "2. They can be used to solve continuous controls problems, unlike DQNs which are limited to discrete action spaces.\n",
    "\n",
    "The Q-learning side of training the critic works similarly to DQNs, except that Q-networks now take states and actions both as inputs to estimate a Q-value, as opposed to simultaneously estimating Q values over all possible discrete actions.\n",
    "\n",
    "The Policy learning in DDPG is fairly simple. We want to learn a deterministic policy $\\pi_{\\theta}(s)$ (parameterized by $\\theta$) which gives the action that maximizes $Q_{\\phi}(s,a)$, a Q-value critic parameterized by $\\phi$. Because the action space is continuous, and we assume the Q-function is differentiable with respect to action, we can just perform gradient ascent (with respect to policy parameters only) to solve\n",
    "\n",
    "$\\max_{\\theta} \\mathbb{E}_{s \\sim {\\mathcal D}} [ Q_{\\phi}(s, \\pi_{\\theta}(s)) ]$\n",
    "\n",
    "Note that the Q-function parameters are treated as constants here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.act_limit * self.pi(obs)\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        act_dim = action_space.shape[0]\n",
    "        act_limit = action_space.high[0]\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)\n",
    "        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.pi(obs).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the homework, we'll test DDPG on a simple inverted pendulum problem.\n",
    "\n",
    "As before, in addition to the main trained networks, we build additional target networks to stabilize training.\n",
    "\n",
    "We will also set the logging directory to be a part of the current working directory but feel free to change that as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory location to save DDPG logs and models\n",
    "ddpg_output_dir = 'ddpg'\n",
    "\n",
    "# Logger setup\n",
    "logger_kwargs={'output_dir':ddpg_output_dir, 'exp_name':'ddpg_pendulum'}\n",
    "logger = EpochLogger(**logger_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we'll be working on for DDPG is the (inverted) [Pendulum](https://gym.openai.com/envs/CartPole-v1/) OpenAI Gym task. \n",
    "\n",
    "The agent observes the angular position and velocity of the pendulum and can apply a continuous-valued torque to try and balance the pendulum.\n",
    "\n",
    "The episode keeps going for up to 200 steps and every step the agent gets reward mainly proportional to how far off it is from balancing the inverted pendulum, with smaller reward components related to how fast the pendulum is moving and how much torque is being applied. The objective is to maximize this reward, which is equivalent to balancing the inverted pendulum quickly and for as long as possible.\n",
    "\n",
    "Additional details for the implementation of this environment can be found via the code [source](https://github.com/openai/gym/blob/4ede9280f9c477f1ca09929d10cdc1e1ba1129f1/gym/envs/classic_control/pendulum.py).\n",
    "\n",
    "A random agent (or a poorly trained one) is unlikely to successfully balance the pendulum, as seen below:\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/pendulum_random_demo.gif\" width=\"360em\">\n",
    "\n",
    "But a well trained agent can successfully solve this problem (using solution code for this HW).\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/pendulum_demo.gif\" width=\"360em\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "steps_per_epoch=4000\n",
    "epochs=20\n",
    "gamma=0.99\n",
    "polyak=0.995\n",
    "batch_size=100\n",
    "start_steps=10000\n",
    "update_after=1000 \n",
    "update_every=50\n",
    "act_noise=0.1\n",
    "num_test_episodes=10\n",
    "max_ep_len=1000\n",
    "save_freq=1\n",
    "\n",
    "pi_lr=1e-3 \n",
    "q_lr=1e-3\n",
    "\n",
    "# Environment Definition\n",
    "env_fn = lambda :gym.make('Pendulum-v0')\n",
    "env, test_env = env_fn(), env_fn()\n",
    "obs_dim = env.observation_space.shape\n",
    "act_dim = env.action_space.shape[0]\n",
    "# Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "act_limit = env.action_space.high[0]\n",
    "\n",
    "# Seeding\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "test_env.seed(seed)\n",
    "\n",
    "# Experience buffer\n",
    "replay_size=int(1e6) \n",
    "replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "# Create actor-critic module and target networks\n",
    "ac = MLPActorCritic(env.observation_space, env.action_space)\n",
    "ac_targ = deepcopy(ac)\n",
    "\n",
    "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "for p in ac_targ.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Set up model saving\n",
    "logger.setup_pytorch_saver(ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can next initialize modified helper functions - similar to the ones before but setup for the different architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def get_action(o, noise_scale):\n",
    "    a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "    a += noise_scale * np.random.randn(act_dim)\n",
    "    return np.clip(a, -act_limit, act_limit)\n",
    "\n",
    "def test_agent():\n",
    "    for j in range(num_test_episodes):\n",
    "        o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "        while not(d or (ep_len == max_ep_len)):\n",
    "            # Take deterministic actions at test time (noise_scale=0)\n",
    "            o, r, d, _ = test_env.step(get_action(o, 0))\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "        logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DDPG Q-learning Loss [10 points]\n",
    "\n",
    "This part is similar to DQN, except you'll need to modify your code to make use of the actor-critic separation appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing DDPG Q-loss\n",
    "def compute_loss_q(data):\n",
    "    o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
    "\n",
    "    \"\"\" STUDENT CODE GOES HERE \"\"\"\n",
    "    q = None # Compute q values for observations as appropriate\n",
    "    loss_q = None\n",
    "    \"\"\" STUDENT CODE ENDS \"\"\"\n",
    "\n",
    "    # Useful info for logging\n",
    "    loss_info = dict(QVals=q.detach().numpy())\n",
    "\n",
    "    return loss_q, loss_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DDPG Policy $\\pi$ Loss [10 points]\n",
    "\n",
    "The policy optimization loss is simple to compute. It can be computed as: $L_\\pi = -Q(o,\\pi(o))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing DDPG pi loss\n",
    "def compute_loss_pi(data):\n",
    "    o = data['obs']\n",
    "    \"\"\" STUDENT CODE GOES HERE \"\"\"\n",
    "    pi_loss = None\n",
    "    \"\"\" STUDENT CODE ENDS \"\"\"\n",
    "    return pi_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall updates are handled similarly to DQN before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizers for policy and q-function\n",
    "pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "q_optimizer = Adam(ac.q.parameters(), lr=q_lr)\n",
    "\n",
    "# Set up model saving\n",
    "logger.setup_pytorch_saver(ac)\n",
    "\n",
    "def update(data):\n",
    "    # First run one gradient descent step for Q.\n",
    "    q_optimizer.zero_grad()\n",
    "    loss_q, loss_info = compute_loss_q(data)\n",
    "    loss_q.backward()\n",
    "    q_optimizer.step()\n",
    "\n",
    "    # Freeze Q-network so you don't waste computational effort \n",
    "    # computing gradients for it during the policy learning step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Next run one gradient descent step for pi.\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi = compute_loss_pi(data)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # Record things\n",
    "    logger.store(LossQ=loss_q.item(), LossPi=loss_pi.item(), **loss_info)\n",
    "\n",
    "    # Finally, update target networks by polyak averaging.\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "            p_targ.data.mul_(polyak)\n",
    "            p_targ.data.add_((1 - polyak) * p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run training.\n",
    "\n",
    "While running the code, pay attention the following logged information to determine if things are progressing well:\n",
    "\n",
    "1. For the first few thousand steps, the agents act randomly so you'll likely see low rewards that don't improve until more than `start_steps` steps have elapsed.\n",
    "2. Average Episode and Test Episode returns should generally be increasing from a very negative value to an average of > -150 for a successful agent.\n",
    "3. The policy loss (LossPi in the log as computed in `compute_loss_pi`) should decrease as the agent gets beter as this reflects how good the policy is at choosing 'good' actions.\n",
    "4. As before, we also expect the Q-value loss to decrease as the agent gets better at estimating Q-values.\n",
    "\n",
    "Unlike DQNs, where everything rides on a good estimation of Q-values, the policy behavior for actor-critic learning relies on critics learning useful Q-value estimations and actors learning useful policies based on the estimated Q-values. \n",
    "The separation offers increased flexibility and often improved representational power (in addition to being able to support continuous-valued actions) but does add extra knobs to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for interaction with environment\n",
    "total_steps = steps_per_epoch * epochs\n",
    "o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "for t in range(total_steps):\n",
    "    \n",
    "    # Until start_steps have elapsed, randomly sample actions\n",
    "    # from a uniform distribution for better exploration. Afterwards, \n",
    "    # use the learned policy (with some noise, via act_noise). \n",
    "    if t > start_steps:\n",
    "        a = get_action(o, act_noise)\n",
    "    else:\n",
    "        a = env.action_space.sample()\n",
    "\n",
    "    # Step the env\n",
    "    o2, r, d, _ = env.step(a)\n",
    "    ep_ret += r\n",
    "    ep_len += 1\n",
    "\n",
    "    # Ignore the \"done\" signal if it comes from hitting the time\n",
    "    # horizon (that is, when it's an artificial terminal signal\n",
    "    # that isn't based on the agent's state)\n",
    "    d = False if ep_len==max_ep_len else d\n",
    "\n",
    "    # Store experience to replay buffer\n",
    "    replay_buffer.store(o, a, r, o2, d)\n",
    "\n",
    "    # Super critical, easy to overlook step: make sure to update \n",
    "    # most recent observation!\n",
    "    o = o2\n",
    "\n",
    "    # End of trajectory handling\n",
    "    if d or (ep_len == max_ep_len):\n",
    "        logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Update handling\n",
    "    if t >= update_after and t % update_every == 0:\n",
    "        for _ in range(update_every):\n",
    "            batch = replay_buffer.sample_batch(batch_size)\n",
    "            update(data=batch)\n",
    "\n",
    "    # End of epoch handling\n",
    "    if (t+1) % steps_per_epoch == 0:\n",
    "        epoch = (t+1) // steps_per_epoch\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs):\n",
    "            logger.save_state({'env': env}, None)\n",
    "\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        test_agent()\n",
    "\n",
    "        # Log info about epoch\n",
    "        logger.log_tabular('Epoch', epoch)\n",
    "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('EpLen', average_only=True)\n",
    "        logger.log_tabular('TestEpLen', average_only=True)\n",
    "        logger.log_tabular('TotalEnvInteracts', t)\n",
    "        logger.log_tabular('QVals', with_min_and_max=True)\n",
    "        logger.log_tabular('LossPi', average_only=True)\n",
    "        logger.log_tabular('LossQ', average_only=True)\n",
    "        logger.dump_tabular()\n",
    "\n",
    "env.close()\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "data = get_datasets(ddpg_output_dir)\n",
    "plot_data(data, xaxis='TotalEnvInteracts', value='Performance', smooth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the trained agents as below (set render=True to visualize - note that additional settings may be required to view on Colab or on a remote server)\n",
    "\n",
    "Note: In order for this code to work, the MLPActorCritic model definition (and dependencies) should be in scope.\n",
    "\n",
    "A solved pendulum controller would typically have EpRet > -150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playback_env, get_action = load_policy_and_env(ddpg_output_dir, 'last', True)\n",
    "run_policy(playback_env, get_action, num_episodes=10, render=False)\n",
    "playback_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tMPpAqRpR2N"
   },
   "source": [
    "## Q2 Sequence to Sequence Modelling with nn.Transformer and Torch Text (20 points)\n",
    "\n",
    "You will implement a part of transformer. This question aims to let you to get familiar with the transformer architecture purposed in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). This question is modified from the original pytorch tutorial [here](https://pytorch.org/tutorials/beginner/transformer_tutorial.html?highlight=transformer), you can refer it when you fill out the code. The general architecture of trasnsformer is shown in the figure below:\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/transformer_architecture.jpg\" width=\"360em\">\n",
    "\n",
    "This question requires you to implement a sequence to sequence model by encoder, which is the left part of the figure. You will use integrated layers in pytorch.\n",
    "\n",
    "The transformer model has been proved to be superior in quality for many sequence-to-sequence\n",
    "problems while being more parallelizable. The ``nn.Transformer`` module\n",
    "relies entirely on an attention mechanism (another module recently\n",
    "implemented as `nn.MultiheadAttention`) to draw global dependencies\n",
    "between input and output. The ``nn.Transformer`` module is now highly\n",
    "modularized such that a single component (like [`nn.TransformerEncoder `](<https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>)\n",
    "in this tutorial) can be easily adapted/composed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkeaGn8INY9k"
   },
   "source": [
    "### Q2.1 Define the model \n",
    "In this question, we train ``nn.TransformerEncoder`` model on a\n",
    "language modeling task. The language modeling task is to assign a\n",
    "probability for the likelihood of a given word (or a sequence of words)\n",
    "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "layer first, followed by a positional encoding layer to account for the order\n",
    "of the word (see the next paragraph for more details). The\n",
    "``nn.TransformerEncoder`` consists of multiple layers of\n",
    "``nn.TransformerEncoderLayer`` . Along with the input sequence, a square\n",
    "attention mask is required because the self-attention layers in\n",
    "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
    "the sequence. For the language modeling task, any tokens on the future\n",
    "positions should be masked. To have the actual words, the output\n",
    "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
    "layer, which is followed by a log-Softmax function. We will see how to implement the ``PositionalEncoding`` in the later question. \n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/encoder.png\" width=\"em\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHI3LBIcgGVO"
   },
   "source": [
    "In the following model, we only train a encoder model, which is the left part of the figure. Then we concatenate a Linear model `self.decoder` to replace the right part of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO AVOID ISSUES WITH PACKAGE IMPORTS, IT MIGHT BE WORTH RESTARTING THE NOTEBOOK KERNEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ai9dTxjUNS5-"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    '''\n",
    "    This is a transformer encoder model, the input arguments are as follows:\n",
    "    args:\n",
    "    ntoken:  dimension of tokens\n",
    "    ninp: dimension of input embeddings\n",
    "    nhid: dimension of the hidden encoding between two layers of TransformerEncoderLayer\n",
    "    nlayers: number of TransformerEncoderLayer layers\n",
    "    nhead: the number of heads in the multiheadattention model\n",
    "    '''\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout) # PositionalEncoding will be implemented in next section.\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        mask = None\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        output = None\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-BxUnw6MZf5"
   },
   "source": [
    "### Q2.2 Positional Encoding\n",
    "#### Q2.2.1 Fill the code block\n",
    "``PositionalEncoding`` module injects some information about the\n",
    "relative or absolute position of the tokens in the sequence. The\n",
    "positional encodings have the same dimension as the embeddings so that\n",
    "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "different frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6A0pUKNMpQ84"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        \"\"\"YOUR CODE ENDS\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        output = None\n",
    "        \"\"\"YOUR CODE ENDS\"\"\"\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7mD6P7eCaKs"
   },
   "source": [
    "#### Q2.2.2 Why do we need this positional encoding in the transformer architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i82IJ2chNstR"
   },
   "source": [
    "### Q2.3 Running the model\n",
    "\n",
    "#### Q2.3.1 Run the code to get desired performance.\n",
    "The training process uses Wikitext-2 dataset from ``torchtext``. The\n",
    "vocab object is built based on the train dataset and is used to numericalize\n",
    "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
    "function arranges the dataset into columns, trimming off any tokens remaining\n",
    "after the data has been divided into batches of size ``batch_size``.\n",
    "For instance, with the alphabet as the sequence (total length of 26)\n",
    "and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
    "length 6:\n",
    "\n",
    "$$\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "  \\end{bmatrix}\n",
    "  \\Rightarrow\n",
    "  \\begin{bmatrix}\n",
    "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "  \\end{bmatrix}\\end{align}\n",
    "$$\n",
    "\n",
    "These columns are treated as independent by the model, which means that\n",
    "the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
    "efficient batch processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**, we're using torchtext v 0.11.0\n",
    "\n",
    "You *may* need to run the following code block below if running locally if errors are thrown about missing packages or components (though check the rest first before going through this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"torchtext==0.11\"\n",
    "!pip install \"spacy>=2.2.4,<=3.2.4\"\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kv4yr-K3OHSs",
    "outputId": "06f95231-18cc-46ba-fd7e-ddf60ef3c774"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.legacy.data.Field(tokenize=get_tokenizer(\"spacy\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.legacy.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjC_fNXWOcIJ"
   },
   "source": [
    "The ``get_batch()`` function generates the input and target sequence for\n",
    "the transformer model. It subdivides the source data into chunks of\n",
    "length ``bptt``. For the language modeling task, the model needs the\n",
    "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
    "weâ€™d get the following two Variables for ``i`` = 0:\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/transformer_input_target1.png\" width=\"em\">\n",
    "<!-- ![](transformer_input_target.png) -->\n",
    "\n",
    "\n",
    "It should be noted that the chunks are along dimension 0, consistent\n",
    "with the ``S`` dimension in the Transformer model. The batch dimension\n",
    "``N`` is along dimension 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxe4vOD8Oh7S"
   },
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ss9NoUZY0Rz"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    print(\"in training loop\")\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 10\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5niRh2AI6qeN"
   },
   "source": [
    "Running the code block below. You will get around 220 ppl on training at the end of epoch 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CB2g1K5ZDBf",
    "outputId": "1ceef4af-b113-41d9-8e0f-f32bc21302b2"
   },
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 1 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"starting\")\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgNfblVjucsp"
   },
   "source": [
    "#### 2.3.2 Why do we need to use `torch.nn.utils.clip_grad_norm_` in training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution]`**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3127b33eef405e17321b7de0d4f8a216c3d52d380e39a853504fe3a37af8cd4"
  },
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
