{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHDIw_DL0SXh"
      },
      "source": [
        "# Problem Set 2 \n",
        "\n",
        "<h4> by Hoang Tran, modified from the version by Xide Xia and Kate Saenko. <br> </h4>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment will introduce you to:\n",
        "1. Understanding the power of ReLU activation.\n",
        "2. Implementing your own autograd.\n",
        "3. Implementing a simple MLP.\n",
        "4. Basic functionality in PyTorch\n",
        "\n",
        "This code has been tested on Colab. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8v9ySzgtD_15"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JkxmpsAj7VK"
      },
      "source": [
        "# Problem 1: Universal approximation power of ReLU networks\n",
        "\n",
        "As we dicussed in class, a two layer NN with sigmoid activation function is a universal approximator, i.e: with sufficient hidden units, it can approximate any real function with desired accuracy. In this problem we want to demonstrate universal approximation power of NNs using ReLU activation units.\n",
        "\n",
        "## **Q1.1** \n",
        "Show that, by composing only 2 hidden units in a ReLU network, i.e. $\\hat{y} = \\sum_{i=1}^2a_i\\ max(0,b_ix+c_i)$, we can build an approximation to the step function $1[x>0]$.\n",
        "The approximator should have value 1 for all values larger than $\\delta$ and increasing linearly for any value between 0 and $\\delta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R35DbojskG6p"
      },
      "source": [
        "**Solution:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "IA5fzqXHD_17",
        "outputId": "bf6fa705-3844-4724-ce68-1cd5e5026e69"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#%%\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#%%\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "#%%\n",
        "def relu2(x, delta):\n",
        "    return np.where(x > delta, 1, x/delta)\n",
        "\n",
        "#%%\n",
        "delta=2\n",
        "\n",
        "#%%\n",
        "x = np.linspace(-5, 5, 300)\n",
        "y = relu(x)\n",
        "y_approx = relu2(y, delta)\n",
        "\n",
        "#%%\n",
        "plt.plot(x, y, label=\"ReLU\")\n",
        "plt.plot(x, y_approx, label=\"ReLU approximation\")\n",
        "plt.scatter([delta],[1])\n",
        "plt.annotate(\"delta\", (delta, 1))\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysSRJ3sbkLMt"
      },
      "source": [
        "## **Q1.2** \n",
        "\n",
        "Show that by composing 4 hidden units in a ReLU network; we can build an approximation to the unit impulse function of duration $\\delta$\n",
        "\n",
        "\\begin{equation}\n",
        "u_\\delta(x) = 1[0\\leq x\\leq \\delta]\n",
        "\\end{equation}\n",
        "\n",
        "The approximator should have value $1$ between $\\frac{\\delta}{4}$ and $\\frac{3\\delta}{4}$ and should be increasing/decreasing on either side of this for a duration of $\\frac{\\delta}{2}$, i.e., it should be 0 for all values less than $\\frac{-\\delta}{4}$ and more than $\\frac{5\\delta}{4}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5wGbZBjkLC1"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "xk-B4I0TD_19",
        "outputId": "e295f831-49ed-4996-b69a-caa7e4226ba8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#%%\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#%%\n",
        "delta=.5\n",
        "\n",
        "#%%\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# def relu0(x):\n",
        "#     return np.maximum(0, x/2)\n",
        "\n",
        "# def relu1(x, delta):\n",
        "#     return np.where(x < (-delta / 4), 1, 0)\n",
        "\n",
        "# def relu1(x, delta):\n",
        "#     return np.where((x+delta/4) > 0, (x+delta/4) * 2/delta, x+delta/4)\n",
        "\n",
        "# def relu2(x, delta):\n",
        "#     #print(x)\n",
        "#     return np.where(x> 4*delta/5, -x + 3, x)\n",
        "\n",
        "# def relu3(x, delta):\n",
        "#     return np.where(x > 1, 1, x)\n",
        "\n",
        "# def relu4(x, delta):\n",
        "#     return np.where(x < 0, 0, x)\n",
        "\n",
        "def relu1(x, delta):\n",
        "    #print(x)\n",
        "    return np.where((x+delta/4) > 0, (x+delta/4) * 2/delta, 0)\n",
        "\n",
        "def relu2(x, delta):\n",
        "    return np.where((x-1) > 1, -(x-3), x)\n",
        "\n",
        "def relu3(x, delta):\n",
        "    return np.where(x > 1, 1, x)\n",
        "\n",
        "def relu4(x, delta):\n",
        "    return np.where(x < 0, 0, x)\n",
        "\n",
        "# def relu1(x, delta):\n",
        "#     return np.maximum(0, (x+delta/4))\n",
        "\n",
        "# def relu2(x, delta):\n",
        "#     #print(x)\n",
        "#     return np.where(x > 0, x * 2/delta, x)\n",
        "\n",
        "#%%\n",
        "x = np.linspace(-0.5*delta, 1.5*delta, 200)\n",
        "y = relu(x)\n",
        "ya = relu1(x, delta)\n",
        "ya = relu2(ya, delta)\n",
        "ya = relu3(ya, delta)\n",
        "ya = relu4(ya, delta)\n",
        "\n",
        "#%%\n",
        "plt.plot(x, y, label=\"ReLU\")\n",
        "plt.plot(x, ya, label=\"Unit Impulse approximation\")\n",
        "plt.scatter([-delta/4, delta/4, (3*delta)/4, (5*delta)/4],[0, 1, 1, 0])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbyanYybkK6C"
      },
      "source": [
        "## **Q1.3** \n",
        "Using your approximator for the unit impulse function in Q1.2, complete the code given bellow to draw the approximator for different duration values $\\delta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bxaR7bemHeg8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "8206e402-6cb2-4731-ca83-cfb1983a354e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def hat_u_delta(x,delta):\n",
        "    return relu4(relu3((relu2((relu1(x, delta)), delta)), delta), delta)\n",
        "\n",
        "def draw_impulse(deltas):\n",
        "    delta_max = max(deltas)\n",
        "    x = np.arange(-delta_max/2, 1.5*delta_max, 0.001).reshape((-1,1))\n",
        "    for delta in deltas:\n",
        "        plt.plot(x,hat_u_delta(x,delta),label='$\\delta$ = '+str(delta))\n",
        "    plt.legend();\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y');\n",
        "\n",
        "draw_impulse([0.5, 0.25, 0.05])\n",
        "# draw_impulse([1,2,4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82j6X2HNkKwy"
      },
      "source": [
        "## **Q1.4**\n",
        "Imagine the idea of Riemann integral, where we approximate the integrand function with unit impulse functions -- see Figure(1) \n",
        "\n",
        "We will approximate the function $f(x)$ defined over $[a,b]$, using N impulse functions as follows:\n",
        "\n",
        "$$\\hat{f}(x) = \\sum_{i=0}^{N-1} f(a + i \\delta)\\, u_\\delta(x-i\\delta), $$\n",
        "where: $$\\delta = \\lfloor \\frac{b-a}{N} \\rfloor$$\n",
        "\n",
        "Complete the code given below using the your implemented approximator in Q1.3 to approximate the $sin(x)$ function over $[0, 2\\pi]$. The code will plot the approximation for different number of impule functions $N$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iLjJuPoZHupW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "895cab36-25b6-44de-f120-9e5fabe166b1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "def f(x):\n",
        "    return np.sin(x)\n",
        "\n",
        "def hat_f(x,N,a,b):\n",
        "    delta = (b-a)/N\n",
        "    total = 0\n",
        "    for i in range(N):\n",
        "        eff = f(a + i * delta)\n",
        "        uimp = hat_u_delta((x - i* delta), delta)\n",
        "        total = total + np.multiply(eff, uimp)\n",
        "    return total\n",
        "\n",
        "def draw_hat_f(N,a,b):\n",
        "    x = np.arange(a, 1.5*b, 0.001).reshape((-1,1))\n",
        "    plt.plot(x,f(x),label='f(x)')\n",
        "    for n in N:\n",
        "        y = hat_f(x,n,a,b)\n",
        "        plt.plot(x,y,label=('N = '+str(n)));\n",
        "    plt.legend(loc = 'upper right')\n",
        "\n",
        "draw_hat_f([3,10,20],0,2*3.15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQtPkwn1kKnf"
      },
      "source": [
        "# Problem 2: Autograd implementation.\n",
        "\n",
        "In class, we discussed the forward and back-propagation in network layers. We pass the input through a network layer and calculate the output of the layer straightforwardly. This step is called forward-propagation. Each layer also implements a function called 'backward'. Backward is responsible for the backward pass of back-propagation. The process of back-propagation follows the schemas: Input -> Forward calls -> Loss function -> derivative -> back-propagation of errors. In neural network, any layer can forward its results to many other layers, in this case, in order to do back-propagation, we sum the deltas coming from all the target layers. \n",
        "\n",
        "In this problem, we will implement both forward and backward for the most commonly used layers including: linear, bias, ReLU, sigmoid, and mean square error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ko0kWtlUjutN"
      },
      "outputs": [],
      "source": [
        "'''backprop implementation with layer abstraction.\n",
        "This could be made more complicated by keeping track of an actual DAG of\n",
        "operations, but this way is not too hard to implement.\n",
        "'''\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    '''A layer in a network.\n",
        "\n",
        "    A layer is simply a function mapping inputs from R^n to R^d for some \n",
        "    specified n and d. A neural network can usually be written as a sequence of \n",
        "    layers -- eg. for input x in R^n, a 3 layer neural network might be:\n",
        "\n",
        "    L3(L2(L1(x)))\n",
        "\n",
        "    We can also view the loss function as itself a layer, so that the loss\n",
        "    of the network is:\n",
        "\n",
        "    Loss(L3(L2(L1(x))))\n",
        "\n",
        "    This class is a base class used to represent different kinds of layer\n",
        "    functions. We will eventually specify a neural network and its loss function\n",
        "    with a list:\n",
        "\n",
        "    [L1, L2, L3, Loss]\n",
        "\n",
        "    where L1, L2, L3, Loss are all Layer objects.\n",
        "\n",
        "    Each Layer object implements a function called 'forward'. forward simply\n",
        "    computes the output of a layer given its input. So instead of\n",
        "    Loss(L3(L2(L1(x))), we write\n",
        "    Loss.forward(L3.forward(L2.forward(L1.forward(x)))).\n",
        "    Doing this computation finishes the forward pass of backprop.\n",
        "\n",
        "    Each layer also implements a function called 'backward'. Backward is\n",
        "    responsible for the backward pass of backprop. After we have computed the\n",
        "    forward pass, we compute\n",
        "    L1.backward(L2.backward(L3.backward(Loss.backward(1))))\n",
        "    We give 1 as the input to Loss.backward because backward is implementing\n",
        "    the chain rule - it multiplies gradients together and so giving 1 as an\n",
        "    input makes the multiplication an identity operation.\n",
        "\n",
        "    The outputs of backward are a little subtle. Some layers may have a\n",
        "    parameter that specifies the function being computed by the layer. For\n",
        "    example, a Linear layer maintains a weight matrix, so that\n",
        "    Linear(x) = xW\n",
        "    for some matrix W.\n",
        "    The input to backward should be the gradient of the final loss with respect\n",
        "    to the output of the current layer. The output of backprop should be the\n",
        "    gradient of the final loss with respect to the input of the current layer,\n",
        "    which is just the output of the previous layer. This is why it is correct\n",
        "    to chain the outputs of backprop together. However, backward should ALSO\n",
        "    compute the gradient of the loss with respect to the current layer's\n",
        "    parameter and store this internally to be used in training.\n",
        "    '''\n",
        "    def __init__(self, parameter=None, name=None):\n",
        "        self.name = name\n",
        "        self.forward_called = False\n",
        "        self.parameter = parameter\n",
        "        self.grad = None\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.grad = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        '''forward pass. Should compute layer and save relevant state\n",
        "        needed for backward pass.\n",
        "        Args:\n",
        "            input: input to this layer.\n",
        "        returns output of operation.\n",
        "        '''\n",
        "        return np.multiply(self.parameter, input)\n",
        "\n",
        "    def backward(self, downstream_grad):\n",
        "        '''Performs backward pass.\n",
        "\n",
        "        This function should also set self.grad to be the gradient of the final\n",
        "        output of the computation with respect to the parameter.\n",
        "\n",
        "        Args:\n",
        "            downstream_grad: gradient from downstream operation in the\n",
        "                computation graph. This package will only consider\n",
        "                computation graphs that result in scalar outputs at the final\n",
        "                node (e.g. loss function computations). As a result,\n",
        "                the dimension of downstream_grad should match the dimension of\n",
        "                the output of this layer.\n",
        "\n",
        "                Formally, if this operation computes F(x), and the final\n",
        "                computation computes a scalar, G(F(x)), then input_grad is\n",
        "                dG/dF.\n",
        "        returns:\n",
        "            gradient to pass to upstream layers. If the layer computes F(x, w),\n",
        "            where x is the input and w is the parameter of the layer, then\n",
        "            the return value should be dF(x,w)/dx * downstream_grad. Here,\n",
        "            x is in R^n, F(x, w) is in R^m, dF(x, w)/dx is a matrix in R^(n x m)\n",
        "            downstream_grad is in R^m and * indicates matrix multiplication.\n",
        "\n",
        "        We should also compute the gradient with respect to the parameter w.\n",
        "        Again by chain rule, this is dF(x, w)/dw * downstream_grad\n",
        "        '''\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VjOyXfWtD_2C"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Code a forward pass\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, parameter=None, name=None):\n",
        "        self.name = name\n",
        "        self.forward_called = False\n",
        "        self.parameter = parameter\n",
        "        self.grad = None\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.grad = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        '''forward pass. Should compute layer and save relevant state\n",
        "        needed for backward pass.\n",
        "        Args:\n",
        "            input: input to this layer.\n",
        "        returns output of operation.\n",
        "        '''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        '''backward pass. Should compute gradient with respect to input\n",
        "        and parameter.\n",
        "        Args:\n",
        "            input: input to this layer.\n",
        "            grad_output: gradient from the next layer.\n",
        "        returns gradient with respect to input.\n",
        "        '''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __call__(self, input):\n",
        "        self.forward_called = True\n",
        "        return self.forward(input)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.name\n",
        "\n",
        "class Linear(Layer):\n",
        "    def __init__(self, input_size, output_size, parameter=None, name='Linear'):\n",
        "        super().__init__(parameter, name)\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        if parameter is None:\n",
        "            self.parameter = {'w': np.random.randn(input_size, output_size), 'b': np.random.randn(output_size)}\n",
        "        else:\n",
        "            self.parameter = parameter\n",
        "    \n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.dot(input, self.parameter['w'\n",
        "        ]) + self.parameter['b']\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        self.grad = {}\n",
        "        self.grad['w'] = np.dot(self.input.T, grad_output)\n",
        "        self.grad['b'] = np.sum(grad_output, axis=0)\n",
        "        self.grad['input'] = np.dot(grad_output, self.parameter['w'].T)\n",
        "        return self.grad['input']\n",
        "\n",
        "class ReLU(Layer):\n",
        "    def __init__(self, name='ReLU'):\n",
        "        super().__init__(name)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.maximum(0, input)\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        relu_grad = input > 0\n",
        "        return grad_output*relu_grad\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self, name='Sigmoid'):\n",
        "        super().__init__(name)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        self.output = 1/(1 + np.exp(-input))\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        sigmoid_grad\n",
        "        return grad_output*sigmoid_grad\n",
        "\n",
        "class MSELoss(Layer):\n",
        "    def __init__(self, name='MSELoss'):\n",
        "        super().__init__(name)\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        self.pred = pred\n",
        "        self.target = target\n",
        "        return np.mean((pred - target)**2)\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        return 2*(self.pred - self.target)/self.pred.size\n",
        "        \n",
        "class Sequential:\n",
        "    def __init__(self, layers=None, name='Sequential'):\n",
        "        self.layers = layers\n",
        "        self.name = name\n",
        "    \n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        for layer in reversed(self.layers):\n",
        "            grad_output = layer.backward(input, grad_output)\n",
        "            input = layer.input\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        for layer in self.layers:\n",
        "            layer.zero_grad()\n",
        "    \n",
        "    def __call__(self, input):\n",
        "        return self.forward(input)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.name\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, lr=0.001, name='SGD'):\n",
        "        self.lr = lr\n",
        "        self.name = name\n",
        "    \n",
        "    def update(self, layer):\n",
        "        if layer.forward_called:\n",
        "            layer.parameter['w'] -= self.lr*layer.grad['w']\n",
        "            layer.parameter['b'] -= self.lr*layer.grad['b']\n",
        "    \n",
        "    def __call__(self, layer):\n",
        "        self.update(layer)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.name\n",
        "\n",
        "class SGDMomentum:\n",
        "    def __init__(self, lr=0.001, momentum=0.99, name='SGDMomentum'):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.name = name\n",
        "        self.velocity = {}\n",
        "    \n",
        "    def update(self, layer):\n",
        "        if layer.forward_called:\n",
        "            if layer not in self.velocity:\n",
        "                self.velocity[layer] = {'w': np.zeros_like(layer.parameter['w']), 'b': np.zeros_like(layer.parameter['b'])}\n",
        "            self.velocity[layer]['w'] = self.momentum*self.velocity[layer]['w'] + (1-self.momentum)*layer.grad['w']\n",
        "            self.velocity[layer]['b'] = self.momentum*self.velocity[layer]['b'] + (1-self.momentum)*layer.grad['b']\n",
        "            layer.parameter['w'] -= self.lr*self.velocity[layer]['w']\n",
        "            layer.parameter['b'] -= self.lr*self.velocity[layer]['b']\n",
        "    \n",
        "    def __call__(self, layer):\n",
        "        self.update(layer)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.name\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, name='Adam'):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.name = name\n",
        "        self.t = 0\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "    \n",
        "    def update(self, layer):\n",
        "        if layer.forward_called:\n",
        "            self.t += 1\n",
        "            if layer not in self.m:\n",
        "                self.m[layer] = {'w': np.zeros_like(layer.parameter['w']), 'b': np.zeros_like(layer.parameter['b'])}\n",
        "            if layer not in self.v:\n",
        "                self.v\n",
        "                self.v[layer] = {'w': np.zeros_like(layer.parameter['w']), 'b': np.zeros_like(layer.parameter['b'])}\n",
        "            self.m[layer]['w'] = self.beta1*self.m[layer]['w'] + (1-self.beta1)*layer.grad['w']\n",
        "            self.m[layer]['b'] = self.beta1*self.m[layer]['b'] + (1-self.beta1)*layer.grad['b']\n",
        "            self.v[layer]['w'] = self.beta2*self.v[layer]['w'] + (1-self.beta2)*(layer.grad['w']**2)\n",
        "            self.v[layer]['b'] = self.beta2*self.v[layer]['b'] + (1-self.beta2)*(layer.grad['b']**2)\n",
        "            m_hat = self.m[layer]['w']/(1-self.beta1**self.t)\n",
        "            v_hat = self.v[layer]['w']/(1-self.beta2**self.t)\n",
        "            layer.parameter['w'] -= self.lr*m_hat/(np.sqrt(v_hat)+self\n",
        "            .epsilon)\n",
        "            m_hat = self.m[layer]['b']/(1-self.beta1**self.t)\n",
        "            v_hat = self.v[layer]['b']/(1-self.beta2**self.t)\n",
        "            layer.parameter['b'] -= self.lr*m_hat/(np.sqrt(v_hat)+self\n",
        "            .epsilon)\n",
        "    \n",
        "    def __call__(self, layer):\n",
        "        self.update(layer)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.name\n",
        "\n",
        "class CrossEntropyLoss(Layer):\n",
        "    def __init__(self, name='CrossEntropyLoss'):\n",
        "        super().__init__(name)\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        self.pred = pred\n",
        "        self.target = target\n",
        "        return -np.sum(target*np.log(pred))/pred.shape[0]\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        return -grad_output*self.target/self.pred\n",
        "\n",
        "class Dropout(Layer):\n",
        "    def __init__(self, p=0.5, name='Dropout'):\n",
        "        super().__init__(name)\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "    \n",
        "    def forward(self, input):\n",
        "        self.mask = np.random.binomial(1, self.p, size=input.shape)\n",
        "        return input*self.mask\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        return grad_output*self.mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKZ3SP2lTpSJ"
      },
      "source": [
        "Below shows an example of the full implementation of the Bias layer, including the forward and backward function. Notice self.grad stores the gradient of the loss with respect to the current layer's parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JF30rs0ITosh"
      },
      "outputs": [],
      "source": [
        "class Bias(Layer):\n",
        "    '''adds a constant bias.'''\n",
        "\n",
        "    def __init__(self, bias, name=\"bias\"):\n",
        "        super(Bias, self).__init__(np.squeeze(bias), name)\n",
        "        self.weights = np.squeeze(bias)\n",
        "\n",
        "    def forward(self, input):  \n",
        "        self.input = input\n",
        "        return self.parameter + self.input\n",
        "\n",
        "    def backward(self, downstream_grad): \n",
        "        self.grad = np.sum(downstream_grad, tuple(range(downstream_grad.ndim - self.parameter.ndim)))\n",
        "        return downstream_grad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntLOrh72mHOW"
      },
      "source": [
        "## **Q2.1** Multiplication layers.\n",
        "\n",
        "Let's start with the basic linear and bias layer. Show the derivatives of linear and bias layer with respect to $X$ respectively.\n",
        "\n",
        "$Z_{linear} = XW$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhpqN_9O2fmz"
      },
      "source": [
        "**Solution (linear layer):**\n",
        "\n",
        "Assume that upstream gradient is computed to be $\\frac{\\partial L}{\\partial Z}$,\n",
        "\n",
        "Apply the chain rule,\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial Z}{\\partial X}\\frac{\\partial L}{\\partial Z} \\hspace{4pc} \\frac{\\partial L}{\\partial W} = \\frac{\\partial Z}{\\partial W}\\frac{\\partial L}{\\partial Z}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gyrJM3O2k2U"
      },
      "source": [
        "Complete the forward and backward function of the linear layer. In backward, you should ALSO set the self.grad to be the gradient of the loss with respect to the current layer's parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4L8Dcfq-lTRd"
      },
      "outputs": [],
      "source": [
        "class Linear(Layer):\n",
        "    '''Linear layer. Parameter is NxM matrix W, input is matrix x of size B x N\n",
        "    where B is batch size, output is xW.'''\n",
        "\n",
        "    def __init__(self, weights, name=\"Linear\"):\n",
        "        super(Linear, self).__init__(weights, name)\n",
        "        self.weights = weights\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.dot(input, self.parameter['w']) + self.parameter['b']\n",
        "    \n",
        "    def backward(self, input, downstream_grad):\n",
        "        self.grad = {}\n",
        "        self.grad['w'] = np.dot(self.input.T, downstream_grad)\n",
        "        self.grad['b'] = np.sum(downstream_grad, axis=0)\n",
        "        self.grad['input'] = np.dot(downstream_grad, self.parameter['w'].T)\n",
        "        return self.grad['input']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icklJkILnhNB"
      },
      "source": [
        "## **Q2.2** Activation layers.\n",
        "\n",
        "Now let's look at the activation layers. Show the derivatives of ReLU and sigmoid. \n",
        "<p>\n",
        "$ReLU(x) = max(0,x)$\n",
        "</p> \n",
        "<p>\n",
        "$\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
        "</p> \n",
        "\n",
        "Hint: Let's assume the gradient of ReLU is 0 when x is 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq5dtn6k49SR"
      },
      "source": [
        "**Solution:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZPP7OTt49NQ"
      },
      "source": [
        "Complete the forward and backward functions. There is no need to update self.grad since there is no parameter in activation layers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FgbalOrBlYkS"
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    '''ReLU layer. No parameters.'''\n",
        "\n",
        "    def __init__(self, name=\"ReLU\"):\n",
        "        super(ReLU, self).__init__(name=name)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.maximum(0, input)\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        relu_grad = input > 0\n",
        "        return grad_output*relu_grad\n",
        "\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    '''Sigmoid layer. No parameters.'''\n",
        "\n",
        "    def __init__(self, name=\"Sigmoid\"):\n",
        "        super(Sigmoid, self).__init__(name=name)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.output = 1/(1 + np.exp(-input))\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        sigmoid_grad\n",
        "        return grad_output*sigmoid_grad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU1LRCr_oI5o"
      },
      "source": [
        "## **Q2.3**  Loss *layers*.\n",
        "Define the mean square error as follows: \n",
        "<p>\n",
        "$MSE(\\hat y) = \\frac{1}{2N}\\sum_{i=1}^N(y_i - \\hat y_i)^2$. \n",
        "</p> \n",
        "where $y$ is the label and $\\hat y$ is your prediction. Show the gradient of MSE w.r.t $\\hat y$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VghiDCb45L4M"
      },
      "source": [
        "**Solution:** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyxKVXh76VIi"
      },
      "source": [
        "Complete the forward and backward functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "47ZAc31L1squ"
      },
      "outputs": [],
      "source": [
        "class MeanSquaredError(Layer):\n",
        "    '''cross entropy loss.'''\n",
        "\n",
        "    def __init__(self, labels, name=\"Mean Squared Error\"):\n",
        "        super(MeanSquaredError, self).__init__(name=\"Mean Squared Error\")\n",
        "        self.labels = labels\n",
        "        \n",
        "    def forward(self, pred, target):\n",
        "        self.pred = pred\n",
        "        self.target = target\n",
        "        return np.mean((pred - target)**2)\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        return 2*(self.pred - self.target)/self.pred.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa8Lz_8Ro5F-"
      },
      "source": [
        "## **Q2.4** \n",
        "\n",
        "Now let's build a simple model using your layers, and compare the autograd results with the numeric derivatives. If everything is implemented in the correct way, the autograd results should be very close to numeric grad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sruJjJSzIYl8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "fa9b2e7d-6852-46cd-a80f-7d059bf9bf05"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-f3422e64fda6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtest_autograd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"looking good!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-f3422e64fda6>\u001b[0m in \u001b[0;36mtest_autograd\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     ]\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mbackward_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     analytics = [np.copy(layer.grad)\n",
            "\u001b[0;32m<ipython-input-12-f3422e64fda6>\u001b[0m in \u001b[0;36mforward_layers\u001b[0;34m(layers, input)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;31m#assert output.size == 1, \"only supports computations that output a scalar!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-533530be9e36>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownstream_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ],
      "source": [
        "# This function computes the derivative numerically using the formula (f(x+delta) - f(x))/delta with small delta. First, we would compute \n",
        "# f(x) which is the original output. Then we perturb the input by a small delta then compute f(x+delta). Finally, we calculate\n",
        "# the difference and divide by delta to get the derivative.\n",
        "def numerical_derivative(layers, input):\n",
        "    base_output = forward_layers(layers, input)\n",
        "    delta = 1e-7\n",
        "    \n",
        "    for layer in layers:\n",
        "        if layer.parameter is None:\n",
        "            continue\n",
        "        size = layer.parameter.size # total number of params\n",
        "        shape = layer.parameter.shape # shape of params\n",
        "        base_param = np.copy(layer.parameter)\n",
        "        perturb = np.zeros(size)\n",
        "        grad = np.zeros(size)\n",
        "         \n",
        "        for i in range(size):\n",
        "            perturb[i] = delta # only current i-th perturb is non-zero\n",
        "            layer.parameter = base_param + np.reshape(perturb, shape) # make a small change (delta) on the i-th parameter\n",
        "            perturb_output = forward_layers(layers, input) # new output after adding a small change (delta) on the i-th parameter\n",
        "            grad[i] = (perturb_output - base_output) / delta # update the grad of i-th parameter\n",
        "            perturb[i] = 0.0 # set it back to zero\n",
        "            \n",
        "        layer.parameter = base_param\n",
        "        layer.grad = np.reshape(np.copy(grad), shape)\n",
        "\n",
        "def forward_layers(layers, input):\n",
        "    '''Forward pass on all the layers. Must be called before backwards pass.'''\n",
        "    output = input\n",
        "    for layer in layers:\n",
        "        output = layer.forward(output)\n",
        "    #assert output.size == 1, \"only supports computations that output a scalar!\"\n",
        "    return output\n",
        "\n",
        "\n",
        "def backward_layers(layers):\n",
        "    '''runs a backward pass on all the layers.\n",
        "    after this function is finished, look at layer.grad to find the\n",
        "    gradient with respect to that layer's parameter.'''\n",
        "    downstream_grad = np.array([1])\n",
        "    for layer in reversed(layers):\n",
        "        downstream_grad = layer.backward(downstream_grad)\n",
        "\n",
        "\n",
        "def zero_grad(layers):\n",
        "    for layer in layers:\n",
        "        layer.zero_grad()\n",
        "\n",
        "        \n",
        "def test_autograd():\n",
        "    h = 2\n",
        "    b = 3\n",
        "    input = np.random.normal(np.zeros((b, h)))\n",
        "    labels = np.array([0,0,1]).reshape(3,1)\n",
        "    layers = [\n",
        "        Linear(np.random.normal(size=(h, 2 * h))),\n",
        "        Sigmoid(),\n",
        "        Bias(np.array([np.random.normal()])),\n",
        "        Linear(np.random.normal(size=(2 * h, 3 * h))),\n",
        "        ReLU(),\n",
        "        Linear(np.random.normal(size=(3 * h, 1))),\n",
        "        MeanSquaredError(labels)\n",
        "    ]\n",
        "    output = forward_layers(layers, input)\n",
        "    backward_layers(layers)\n",
        "    analytics = [np.copy(layer.grad)\n",
        "                 for layer in layers if layer.grad is not None]\n",
        "    zero_grad(layers)\n",
        "\n",
        "    numerical_derivative(layers, input)\n",
        "    numerics = [np.copy(layer.grad)\n",
        "                for layer in layers if layer.grad is not None]  \n",
        "    # Computing the difference between the derivative of our implemented function and the numerical derivative \n",
        "    diff = np.sum([np.linalg.norm(analytic - numeric)/np.linalg.norm(numeric)\n",
        "                   for analytic, numeric in zip(analytics, numerics)])\n",
        "    \n",
        "    assert diff < 1e-5, \"autograd differs by {} from numeric grad!\".format(diff)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_autograd()\n",
        "    print(\"looking good!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y613iZJ0pjO2"
      },
      "source": [
        "## Problem 3: Implementing a simple MLP.\n",
        "\n",
        "In this problem we will develop a neural network with fully-connected layers, aka Multi-Layer Perceptron (MLP) using the layers from Problem 2. Below, we initialize toy data  that we will use to develop your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD5cHE09pVAr",
        "outputId": "52359668-2801-4e99-830f-f550ba39d4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X =  (100, 1)\n",
            "y =  (100, 1)\n"
          ]
        }
      ],
      "source": [
        "# setup\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create some toy data\n",
        "X = np.linspace(-1, 1, 100).reshape(-1,1)\n",
        "y = 5*X + 2 + 0.5*np.random.normal() # noisy y\n",
        "\n",
        "print ('X = ', X.shape)\n",
        "print('y = ', y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtBLq2v7EiUK"
      },
      "source": [
        "We will use the following class `TwoLayerMLP` to implement our network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HM1ia9y0RlUF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "b4ebbb74-ed50-47d5-bbb4-5f7a5602c73e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-c01b6d3efae1>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    self.params['W1'] = ## -- ! code required\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "class TwoLayerMLP(object):\n",
        "    def __init__(self, input_size, hidden_size, label_size, std=1e-1, activation='sigmoid'):\n",
        "        np.random.seed(0)\n",
        "        self.input_size = input_size\n",
        "        self.label_size = label_size\n",
        "        \n",
        "        self.params = {}\n",
        "\n",
        "        ## TODO: Initialize your parameters below using input_size, hidden_size, label_size\n",
        "        ## the weights of the linear layers are normally distributed with standard deviation = std\n",
        "        ## and mean = 0. The bias is zero. The structure of the network is as follows:\n",
        "        ## linear1 -> bias1 -> sigmoid -> linear 2 -> bias 2\n",
        "        \n",
        "\n",
        "        self.params['W1'] = ## -- ! code required  \n",
        "        self.params['W2'] = ## -- ! code required  \n",
        "        self.params['b1'] = ## -- ! code required  \n",
        "        self.params['b2']  = ## -- ! code required  \n",
        "        ###########################################################################\n",
        "        #                            END OF YOUR CODE\n",
        "        ###########################################################################\n",
        "        self.activation = 'sigmoid'\n",
        "        # Define the model \n",
        "        self.models = [\n",
        "                  Linear(self.params['W1']),\n",
        "                  Bias(self.params['b1']),\n",
        "                  Sigmoid(),\n",
        "                  Linear(self.params['W2']),\n",
        "                  Bias(self.params['b2'])\n",
        "                ]    \n",
        "           \n",
        "    def loss(self, X, y=None, reg=0.0):\n",
        "        # Unpack variables from the params dictionary\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "        _, C = W2.shape\n",
        "        N, D = X.shape\n",
        "\n",
        "        ## TODO: Finish the forward pass, and compute the loss using the layers and the loss\n",
        "        ## layer in problem 2\n",
        "        ## -- ! code required  \n",
        "            \n",
        "\n",
        "            \n",
        "\n",
        "        grads = {}\n",
        "        ###########################################################################\n",
        "        # TODO: Compute the backward pass, computing the derivatives of the weights\n",
        "        # and biases. Store the results in the grads dictionary. For example,\n",
        "        # grads['W1'] should store the gradient on W1, and be a matrix of same size\n",
        "        ###########################################################################\n",
        "        ## -- ! code required  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                            END OF YOUR CODE\n",
        "        ###########################################################################\n",
        "        return loss, grads\n",
        "\n",
        "    def backward_layers(self, downstream_grad):\n",
        "        '''runs a backward pass on all the layers.\n",
        "        after this function is finished, look at layer.grad to find the\n",
        "        gradient with respect to that layer's parameter.'''\n",
        "        for layer in reversed(self.models):\n",
        "            downstream_grad = layer.backward(downstream_grad)\n",
        "\n",
        "    def train(self, X, y, X_val, y_val,\n",
        "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
        "            reg=1e-5, num_epochs=10,\n",
        "            batch_size=1, verbose=False):\n",
        "\n",
        "        num_train = X.shape[0]\n",
        "        iterations_per_epoch = 1 \n",
        "        epoch_num = 0\n",
        "\n",
        "        # Use SGD to optimize the parameters in self.model\n",
        "        loss_history = []\n",
        "        grad_magnitude_history = []\n",
        "        train_acc_history = []\n",
        "        val_acc_history = []\n",
        "\n",
        "        np.random.seed(1)\n",
        "        for epoch in range(num_epochs):\n",
        "            # fixed permutation (within this epoch) of training data\n",
        "            perm = np.random.permutation(num_train)\n",
        "\n",
        "            # go through minibatches\n",
        "            for it in range(iterations_per_epoch):\n",
        "                X_batch = None\n",
        "                y_batch = None\n",
        "\n",
        "                # Create a random minibatch\n",
        "                idx = perm[it*batch_size:(it+1)*batch_size]\n",
        "                X_batch = X[idx, :]\n",
        "                y_batch = y[idx]\n",
        "                # Compute loss and gradients using the current minibatch\n",
        "                loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
        "                #print(\"loss\", loss)\n",
        "                loss_history.append(loss)\n",
        "\n",
        "                # do gradient descent\n",
        "                for param in self.params:\n",
        "                    self.params[param] -= grads[param] * learning_rate\n",
        "\n",
        "                # record gradient magnitude (Frobenius) for W1\n",
        "                grad_magnitude_history.append(np.linalg.norm(grads['W1']))\n",
        "\n",
        "            # Decay learning rate\n",
        "            learning_rate *= learning_rate_decay\n",
        "\n",
        "        return {\n",
        "          'loss_history': loss_history,\n",
        "          'grad_magnitude_history': grad_magnitude_history, \n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSkRivC0soqb"
      },
      "source": [
        "### Q3.1 Forward pass\n",
        "\n",
        "Our 2-layer MLP uses a mean squared error loss layer defined in Problem 2.\n",
        "\n",
        "Please take a look at method `TwoLayerMLP.loss`. This function takes in the data and weight parameters, and computes the class scores (output of the forward layer), the loss ($L$), and the gradients on the parameters. \n",
        "\n",
        "- Use the layers designed in **Problem 2** and implement the first part of the function to compute `scores` and `loss`. Afterwards, run the following two test cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQPe4gxUI-2_"
      },
      "outputs": [],
      "source": [
        "input_size = 1\n",
        "hidden_size = 10\n",
        "label_size = 1\n",
        "\n",
        "net = TwoLayerMLP(input_size, hidden_size, label_size)\n",
        "scores = forward_layers(net.models, X)\n",
        "print ('(1) Your scores:\\n')\n",
        "print (np.linalg.norm(scores))\n",
        "print ('\\n')\n",
        "correct_norm = 2.00385\n",
        "# # The difference should be very small (< 1e-4)\n",
        "print ('Difference between your scores and correct scores:')\n",
        "print (np.sum(np.abs(np.linalg.norm(scores) -correct_norm)))\n",
        "print ('\\n')\n",
        "\n",
        "loss, _ = net.loss(X, y, reg=0.1)\n",
        "correct_loss = 5\n",
        "\n",
        "# Since we generate random data, your loss would not be the same as the correct loss.\n",
        "# However, the difference should be fairly small (less than 1 or 2)\n",
        "print ('(2) Your loss: %f'%(loss))\n",
        "print ('Difference between your loss and correct loss:')\n",
        "print (np.sum(np.abs(loss - correct_loss)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9cchDnkEX6I"
      },
      "source": [
        "## **Q3.2** Backward pass\n",
        "- Implement the second part to compute gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`, stored in `grads`. \n",
        "\n",
        "Hint: you can quickly get the gradients with respect to parameters by calling **self.backward_layers**(downstream_grad).\n",
        "\n",
        "Now debug your backward pass using a numeric gradient check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bJ4C4isJMxY"
      },
      "outputs": [],
      "source": [
        "# Use numeric gradient checking to check your implementation of the backward pass.\n",
        "# If your implementation is correct, the difference between the numeric and\n",
        "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
        "\n",
        "\n",
        "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
        "  \"\"\" \n",
        "  a naive implementation of numerical gradient of f at x \n",
        "  - f should be a function that takes a single argument\n",
        "  - x is the point (numpy array) to evaluate the gradient at\n",
        "  \"\"\" \n",
        "\n",
        "  fx = f(x) # evaluate function value at original point\n",
        "  grad = np.zeros_like(x)\n",
        "  # iterate over all indexes in x\n",
        "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "  while not it.finished:\n",
        "\n",
        "    # evaluate function at x+h\n",
        "    ix = it.multi_index\n",
        "    oldval = x[ix]\n",
        "    x[ix] = oldval + h # increment by h\n",
        "    fxph = f(x) # evalute f(x + h)\n",
        "    x[ix] = oldval - h\n",
        "    fxmh = f(x) # evaluate f(x - h)\n",
        "    x[ix] = oldval # restore\n",
        "\n",
        "    # compute the partial derivative with centered formula\n",
        "    grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
        "    if verbose:\n",
        "      print (ix, grad[ix])\n",
        "    it.iternext() # step to next dimension\n",
        "\n",
        "  return grad\n",
        "\n",
        "loss, grads = net.loss(X, y, reg=0.1)\n",
        "\n",
        "# these should all be very small\n",
        "for param_name in grads:\n",
        "    f = lambda W: net.loss(X, y, reg=0.1)[0]\n",
        "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
        "    print ('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4So3UarqGLbl"
      },
      "source": [
        "## **Q3.3** Train the Sigmoid network\n",
        "To train the network we will use stochastic gradient descent (SGD), implemented in `TwoLayerNet.train`. Train the two-layer network and plot the ['loss_history']. We don't expect you to optimize the training process. As long as the the loss graph looks reasonable (loss is going down), you will get full credits. All the current hyperparameters are set to 1 so feel free to play around with these values. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtnITSRP4Z9k"
      },
      "outputs": [],
      "source": [
        "stats = net.train(X, y, X, y,learning_rate=1, reg=1e-5, batch_size = 1, num_epochs=1, verbose=False)\n",
        "## TODO: Plot ['loss_history'] here\n",
        "## -- ! code required \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyQVjefVIwcQ"
      },
      "source": [
        "# Problem 4: Pytorch Intro\n",
        "## **Q4.0**: Pytorch tutorials\n",
        "This homework will introduce you to [PyTorch](https://pytorch.org), currently the fastest growing deep learning library, and the one we will use in this course.\n",
        "\n",
        "Before starting the homework, please go over these introductory tutorials on the PyTorch webpage:\n",
        "\n",
        "*   [60-minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "p9GI4YuDL-8D"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxw9G9oyI67G"
      },
      "source": [
        "The `torch.Tensor` class is the basic building block in PyTorch and is used to hold data and parameters. The `autograd` package provides automatic differentiation for all operations on Tensors. After reading about Autograd in the tutorials above,  we will implement a few simple examples of what Autograd can do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9kS5w0m4pAv"
      },
      "source": [
        "## **Q4.1**. Simple function\n",
        " Use `autograd` to do backpropagation on a simple function, $f=(x+y)*z$. \n",
        "\n",
        "**Q4.1.1** Create three inputs with values $x=-2$, $y=5$ and $z=-4$ as tensors and set `requires_grad=True` to track computation on them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rzYKPuxYJkzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f6392b-2e1a-42e7-e18a-c8463da2e27f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "## -- ! code required\n",
        "\n",
        "x = torch.tensor([-2.], requires_grad=True)\n",
        "y = torch.tensor([5.], requires_grad=True)\n",
        "z = torch.tensor([-4.], requires_grad=True)\n",
        "f = (x + y) * z\n",
        "\n",
        "print(f.backward())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiEkkQJH4tah"
      },
      "source": [
        "**Q4.1.2** Compute the $q=x+y$ and $f=q \\times z$ functions, creating tensors for them in the process. Print out $q,f$, then run `f.backward(retain_graph=True)`, to compute the gradients w.r.t. $x,y,z$. The `retain_graph` attribute tells autograd to keep the computation graph around after the backward pass as opposed deleting it (freeing some memory). Print the gradients. Note that the gradient for $q$ will be `None` since it is an intermediate node, even though `requires_grad` for it is automatically set to `True`. To access gradients for intermediate nodes in PyTorch you can use hooks as mentioned in [this answer](https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94/2). Compute the values by hand to verify your solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GbpjkpQJwQp"
      },
      "outputs": [],
      "source": [
        "## -- ! code required\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUFLoE6d4xQU"
      },
      "source": [
        "**Q4.1.3** If we now run `backward()` again, it will add the gradients to their previous values. Try it by running the above cell multiple times. This is useful in some cases, but if we just wanted to re-compute the gradients again, we need to zero them first, then run `backward()`. Add this step, then try running the  backward function multiple times to make sure the answer is the same each time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ach6UcqhJyGS"
      },
      "outputs": [],
      "source": [
        "## -- ! code required\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXD55v9vMkC"
      },
      "source": [
        "## **Q4.2** Neuron\n",
        " Implement the function corresponding to one neuron (logistic regression unit) that we saw in the lecture and compute the gradient w.r.t. $x$ and $w$. The function is $f=\\sigma(w^Tx)$ where $\\sigma()$ is the sigmoid function. Initialize $x=[-1, -2, 1]$ and the weights to $w=[2, -3, -3]$ where $w_3$ is the bias. Print out the gradients and double check their values by hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P3XfXjYJ1TE"
      },
      "outputs": [],
      "source": [
        "## -- ! code required\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTve24kfvRVG"
      },
      "source": [
        "## **Q4.3**. torch.nn\n",
        " We will now implement the same neuron function $f$ with the same variable values as in Q4.2, but using the `Linear` class from `torch.nn`, followed by the [Sigmoid](https://pytorch.org/docs/stable/nn.html#torch.nn.Sigmoid) class. In general, many useful functions are already implemented for us in this package. Compute the gradients $\\partial f/\\partial w$ by running `backward()` and print them out (they will be stored in the Linear variable, e.g. in `.weight.grad`.) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGvU99m9J3u9"
      },
      "outputs": [],
      "source": [
        "## -- ! code required\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj_uO0_dvVk5"
      },
      "source": [
        "## **Q4.4** Module\n",
        " Now lets put these two functions (Linear and Sigmoid) together into a \"module\". Read the [Neural Networks tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) if you have not already.\n",
        "\n",
        "**Q4.4.1** Make a subclass of the `Module` class, called `Neuron`. Set variables to the same values as above. You will need to define the `__init__` and `forward` methods. Our neuron would have 1 linear layer and 1 sigmoid layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcTn1__FvXVy"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Neuron(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Neuron, self).__init__()\n",
        "       ## -- ! code required\n",
        "\n",
        "    def forward(self, x):\n",
        "       ## -- ! code required\n",
        "        return x\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bum-YdWsvZ_k"
      },
      "source": [
        "**Q4.4.2** Now create a  variable of your `Neuron` class called `my_neuron` and run backpropagation on it. Print out the gradients again. Make sure you zero out the gradients first, by calling `.zero_grad()` function of the parent class. Even if you will not re-compute the backprop, it is good practice to do this every time to avoid accumulating gradient!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5NIlAErJ8VK"
      },
      "outputs": [],
      "source": [
        "## -- ! code required"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Qm62nKvhJZ"
      },
      "source": [
        "## **Q4.5**. Loss and SGD\n",
        " Now, lets train our neuron on some data. The code below creates a toy dataset containing a few inputs $x$ and outputs $y$ (a binary 0/1 label), as well as a function that plots the data and current solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq3jAoO8vilS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create some toy 2-D datapoints with binary (0/1) labels\n",
        "x = torch.tensor([[1.2, 1], [0.2, 1.4], [0.5, 0.5], \n",
        "                  [-1.5, -1.3], [0.2, -1.4], [-0.7, -0.5]])\n",
        "y = torch.tensor([0, 0, 0, 1, 1, 1 ])\n",
        "\n",
        "def plot_soln(x, y, params):\n",
        "  plt.plot(x[y==1,0], x[y==1,1], 'r+')\n",
        "  plt.plot(x[y==0,0], x[y==0,1], 'b.')\n",
        "  plt.grid(True)\n",
        "  plt.axis([-2, 2, -2, 2])\n",
        "  \n",
        "  # NOTE : This may depend on how you implement Neuron.\n",
        "  #   Change accordingly\n",
        "  w0 = params[0][0][0].item()\n",
        "  w1 = params[0][0][1].item()\n",
        "  bias = params[1][0].item()\n",
        "  \n",
        "  print(\"w0 =\", w0, \"w1 =\", w1, \"bias =\", bias)\n",
        "  dbx = torch.tensor([-2, 2])\n",
        "  dby = -(1/w1)*(w0*dbx + bias)  # plot the line corresponding to the weights and bias\n",
        "  plt.plot(dbx, dby)\n",
        "\n",
        "params = list(my_neuron.parameters())\n",
        "plot_soln(x, y, params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOYoZkq1vkxt"
      },
      "source": [
        "**Q4.5.1** Declare an object `criterion` of type `nn.CrossEntropyLoss`. Note that this can be called as a function on two tensors, one representing the network outputs and the other, the targets that the network is being trained to predict, to return the loss. Print the value of the loss on the dataset using the initial weights and bias defined above in Q4.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZBNql1CLUoo"
      },
      "outputs": [],
      "source": [
        "## -- ! code required\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJFVZfscvtix"
      },
      "source": [
        "**Q4.5.2** Print out the chain of `grad_fn` functions backwards starting from `loss.grad_fn`  to demonstrate what backpropagation will be run on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbfZKQpzLtRh"
      },
      "outputs": [],
      "source": [
        "## -- ! code required\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdX99YRGvxiU"
      },
      "source": [
        "**Q4.5.3** Run the Stochastic Gradient Descent (SGD) optimizer from the `torch.optim` package to train your classifier on the toy dataset. Use the entire dataset in each batch. Use a learning rate of $0.01$ (no other hyperparameters). You will need to write a training loop that uses the `.step()` function of the optimizer. Plot the solution and print the loss after 1000 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5uCm4KcL5K5"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(my_neuron.parameters(), lr=0.01)\n",
        "\n",
        "# training loop\n",
        "for i in range(10000):\n",
        "  ## -- ! code required\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"loss =\", loss.item())\n",
        "params = list(my_neuron.parameters())\n",
        "plot_soln(x, y, params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed2aFhiYK-vb"
      },
      "source": [
        "**Q4.5.4** How many thousands of iterations does it take (approximately) until the neuron learns to classify the data correctly?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tuMoY6JLG_C"
      },
      "source": [
        "***Solution***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MdGRt1jOmBX"
      },
      "source": [
        "## **Q4.6**. Hidden space ablation\n",
        "\n",
        "Now let's look at the size of network's hidden space. We will create and train a **2-layer MLP** network on the [SVHN Dataset](http://ufldl.stanford.edu/housenumbers/).\n",
        "\n",
        "The SVHN dataset consists of photos of house numbers, collected automatically using Google's Street View. Recognizing multi-digit numbers in photographs captured at street level is an important component of modern-day map making. Google’s Street View imagery contains hundreds of millions of geo-located 360 degree panoramic images. The ability to automatically transcribe an address number from a geo-located patch of pixels and associate the transcribed number with a known street address helps pinpoint, with a high degree of accuracy, the location of the building it represents. Below are example images from the dataset. Note that for this dataset, each image (32x32 pixels) has been cropped around a single number in its center, which is the number we want to classify.\n",
        "\n",
        "![SVHN images](http://ufldl.stanford.edu/housenumbers/32x32eg.png)\n",
        "\n",
        "In this problem, we turn the input images into grayscale and then flat them into 1-D vector. First, download the SVHN dataset using `torchvision` and display the images in the first batch. Take a look at the [Training a Classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) tutorial for an example. Follow the settings used there, such as the normalization, batch size of 4 for the `torch.utils.data.DataLoader`, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtdl8h1eY2dZ"
      },
      "outputs": [],
      "source": [
        "# solution here\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Grayscale(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(root='./data', split='train', transform=transform, download=True)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBDzvc_nLfGJ"
      },
      "source": [
        "### Q4.6.1 2-layer MLP \n",
        "\n",
        "Next, we will train a 2-layer MLP on the data. We have defined a simple 2-layer MLP for you with two fc layers and ReLU activation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWu68j0AaYD1"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Neuron(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Neuron, self).__init__()\n",
        "        self.l1 = nn.Linear(1024, hidden_size)\n",
        "        self.l2 = nn.Linear(hidden_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1024)\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = self.l2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7OMjOH0MXpR"
      },
      "source": [
        "You can check the number of parameters in the model by printing out the model summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyODwJ0rJYq1"
      },
      "outputs": [],
      "source": [
        "def model_summary(model):\n",
        "  print(\"model_summary\")\n",
        "  print()\n",
        "  print(\"Layer_name\"+\"\\t\"*7+\"Number of Parameters\")\n",
        "  print(\"=\"*100)\n",
        "  model_parameters = [layer for layer in model.parameters() if layer.requires_grad]\n",
        "  layer_name = [child for child in model.children()]\n",
        "  j = 0\n",
        "  total_params = 0\n",
        "  print(\"\\t\"*10)\n",
        "  for i in layer_name:\n",
        "    print()\n",
        "    param = 0\n",
        "    try:\n",
        "      bias = (i.bias is not None)\n",
        "    except:\n",
        "      bias = False  \n",
        "    if not bias:\n",
        "      param =model_parameters[j].numel()+model_parameters[j+1].numel()\n",
        "      j = j+2\n",
        "    else:\n",
        "      param =model_parameters[j].numel()\n",
        "      j = j+1\n",
        "    print(str(i)+\"\\t\"*3+str(param))\n",
        "    total_params+=param\n",
        "  print(\"=\"*100)\n",
        "  print(f\"Total Params:{total_params}\")       \n",
        "\n",
        "my_neuron = Neuron(10)\n",
        "model_summary(my_neuron)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOqfDwW7MmCM"
      },
      "source": [
        "Instantiate the cross-entropy loss `criterion`, and an SGD optimizer from the `torch.optim` package with learning rate $.001$ and momentum $.9$. You may also want to enable GPU training using `torch.device()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m60vdKuIMIM1"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "## -- ! code required\n",
        "criterion = None\n",
        "optimizer = None\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# if we set the hardware to GPU in the Notebook settings, this should print a CUDA device:\n",
        "print(device)\n",
        "\n",
        "my_neuron.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnYmjlVnM-a0"
      },
      "source": [
        "### Q4.6.2 Training\n",
        "Complete the training loop that makes five full passes through the dataset (five epochs) using SGD. Your batch size should be 4 and hidden size is 10. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVc3vjnsZYD-"
      },
      "outputs": [],
      "source": [
        "my_neuron = Neuron(10)\n",
        "\n",
        "# num of epoch\n",
        "stats = []\n",
        "for epoch in range(5):\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    ## -- ! code required  \n",
        "    \n",
        "\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "        print('[%d, %5d] loss: %.3f' %\n",
        "              (epoch + 1, i + 1, running_loss / 2000))\n",
        "        stats.append(running_loss / 2000)\n",
        "        running_loss = 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCnLhyMWNdU9"
      },
      "source": [
        "Train the model again but this time set the hidden size as 100. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX8xQGf4Zayy"
      },
      "outputs": [],
      "source": [
        "my_neuron_large = Neuron(100)\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = ## -- ! code required \n",
        "\n",
        "# num of epoch\n",
        "stats_v2 = []\n",
        "for epoch in range(5):\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    inputs, labels = data\n",
        "    #inputs, labels = inputs.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()   # zero the gradient buffers\n",
        "    outputs = my_neuron_large(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "        print('[%d, %5d] loss: %.3f' %\n",
        "              (epoch + 1, i + 1, running_loss / 2000))\n",
        "        stats_v2.append(running_loss / 2000)\n",
        "        running_loss = 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cGSgfUiNn63"
      },
      "source": [
        "### Q4.6.3\n",
        "\n",
        "Plot the loss of the model with hidden_size=10 with hidden_size=100 and compare the performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li05qkZ-ZfzN"
      },
      "outputs": [],
      "source": [
        "## -- ! code required \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXCDFdORONV2"
      },
      "source": [
        "**Solution here:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhLVHz1oMylI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs523",
      "language": "python",
      "name": "cs523"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}