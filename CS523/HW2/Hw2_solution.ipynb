{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHDIw_DL0SXh"
   },
   "source": [
    "# Problem Set 2 \n",
    "\n",
    "<h4> by Hoang Tran, modified from the version by Xide Xia and Kate Saenko. <br> </h4>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "This assignment will introduce you to:\n",
    "1. Understanding the power of ReLU activation.\n",
    "2. Implementing your own autograd.\n",
    "3. Implementing a simple MLP.\n",
    "4. Basic functionality in PyTorch\n",
    "\n",
    "This code has been tested on Colab. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JkxmpsAj7VK"
   },
   "source": [
    "# Problem 1: Universal approximation power of ReLU networks\n",
    "\n",
    "As we dicussed in class, a two layer NN with sigmoid activation function is a universal approximator, i.e: with sufficient hidden units, it can approximate any real function with desired accuracy. In this problem we want to demonstrate universal approximation power of NNs using ReLU activation units.\n",
    "\n",
    "## **Q1.1** \n",
    "Show that, by composing only 2 hidden units in a ReLU network, i.e. $\\hat{y} = \\sum_{i=1}^2a_i\\ max(0,b_ix+c_i)$, we can build an approximation to the step function $1[x>0]$.\n",
    "The approximator should have value 1 for all values larger than some positive value $\\delta$ and decreasing linearly for any value between 0 and $\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R35DbojskG6p"
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "$y = \\frac{1}{\\delta}(max(0,x) - max(0,x-\\delta))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysSRJ3sbkLMt"
   },
   "source": [
    "## **Q1.2** \n",
    "\n",
    "Show that by composing 4 hidden units in a ReLU network; we can build an approximation to the unit impulse function of duration $\\delta$\n",
    "\n",
    "\\begin{equation}\n",
    "u_\\delta(x) = 1[0\\leq x\\leq \\delta]\n",
    "\\end{equation}\n",
    "\n",
    "The approximator should have value $1$ between $\\frac{\\delta}{4}$ and $\\frac{3\\delta}{4}$ and should be increasing/decreasing on either side of this for a duration of $\\frac{\\delta}{2}$, i.e., it should be 0 for all values less than $\\frac{-\\delta}{4}$ and more than $\\frac{5\\delta}{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5wGbZBjkLC1"
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "$y = \\frac{2}{\\delta}(max(0,x+\\frac{\\delta}{4}) - max(0,x-\\frac{\\delta}{4}) - max(0,x-\\frac{3\\delta}{4}) + max(0,x-\\frac{5\\delta}{4}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbyanYybkK6C"
   },
   "source": [
    "## **Q1.3** \n",
    "Using your approximator for the unit impulse function in Q3.2, complete the code given bellow to draw the approximator for different duration values $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "KoCRLBNcktn8",
    "outputId": "268c0ebd-08ae-480f-900e-b10a8735d88c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "def hat_u_delta(x,delta):\n",
    "    ## -- ! code required   \n",
    "    y = 2/delta* (relu(x+delta/4.) - relu(x-delta/4.) - relu(x-3*delta/4.) + relu(x-5*delta/4.)) \n",
    "    return y\n",
    "\n",
    "def draw_impulse(deltas):\n",
    "    x = np.arange(-2, 2, 0.01).reshape((-1,1))\n",
    "    for delta in deltas:\n",
    "        plt.plot(x,hat_u_delta(x,delta))\n",
    "    plt.legend(['$\\delta$ = 0.5', '$\\delta$ = 0.25', '$\\delta$ = 0.05']);\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y');\n",
    "\n",
    "draw_impulse([0.5, 0.25, 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82j6X2HNkKwy"
   },
   "source": [
    "## **Q1.4**\n",
    "Imagine the idea of riemann integral, where we approximate the integrand function with unit impulse functions (fig(1)). \n",
    "We will approximate the function $f(x)$ defined over $[a,b]$, using N impulse functions as follows:\n",
    "\n",
    "$$\\hat{f(x)} = \\sum_{i=0}^{N-1} f(a + i \\delta)\\, u_\\delta(x-i\\delta), $$\n",
    "where: $$\\delta = \\lfloor \\frac{b-a}{N} \\rfloor$$\n",
    "\n",
    "![riemann-gif](https://drive.google.com/uc?id=1nY1BHbbEpdm7OE3USfc0BhbbEWlTmsvv)\n",
    "\n",
    "Using your implemented approximator for unit impulse function in Q3.2; complete the code given bellow to approximate the $sin(x)$ function over $[0, 2\\pi]$. The code will plot the approximation for different number of impule functions $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "_IhOmYTMkVAt",
    "outputId": "41b0e3cb-b473-468e-8cdb-919905a44c1b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def hat_f(x,N,a,b):\n",
    "    y = np.zeros(x.shape)\n",
    "    delta = (b-a)/N\n",
    "    for i in range(N):\n",
    "        y += f(i*delta+a)* hat_u_delta(x-i*delta,delta) ## -- ! code required   \n",
    "    return y\n",
    "\n",
    "def draw_hat_f(N,a,b):\n",
    "    x = np.arange(a, 1.5*b, 0.01).reshape((-1,1))\n",
    "    plt.plot(x,f(x))\n",
    "    for n in N:\n",
    "        y = hat_f(x,n,a,b)\n",
    "        plt.plot(x,y);\n",
    "    plt.legend([\"f(x)\"]+[\"N = \" + str(n) for n in N],loc = 'upper right')\n",
    "\n",
    "draw_hat_f([3,10,20],0,2*3.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQtPkwn1kKnf"
   },
   "source": [
    "# Problem 2: Autograd implementation.\n",
    "\n",
    "In class, we discussed the forward and back-propagation in network layers. We pass the input through a network layer and calculate the output of the layer straightforwardly. This step is called forward-propagation. Each layer also implements a function called 'backward'. Backward is responsible for the backward pass of back-propagation. The process of back-propagation follows the schemas: Input -> Forward calls -> Loss function -> derivative -> back-propagation of errors. In neural network, any layer can forward its results to many other layers, in this case, in order to do back-propagation, we sum the deltas coming from all the target layers. \n",
    "\n",
    "In this problem, we will implement both forward and backward for the most commonly used layers including: linear, bias, ReLU, sigmoid, and mean square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ko0kWtlUjutN"
   },
   "outputs": [],
   "source": [
    "'''backprop implementation with layer abstraction.\n",
    "This could be made more complicated by keeping track of an actual DAG of\n",
    "operations, but this way is not too hard to implement.\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    '''A layer in a network.\n",
    "\n",
    "    A layer is simply a function from R^n to R^d for some specified n and d.\n",
    "    A neural network can usually be written as a sequence of layers:\n",
    "    if the original input x is in R^d, a 3 layer neural network might be:\n",
    "\n",
    "    L3(L2(L1(x)))\n",
    "\n",
    "    We can also view the loss function as itself a layer, so that the loss\n",
    "    of the network is:\n",
    "\n",
    "    Loss(L3(L2(L1(x))))\n",
    "\n",
    "    This class is a base class used to represent different kinds of layer\n",
    "    functions. We will eventually specify a neural network and its loss function\n",
    "    with a list:\n",
    "\n",
    "    [L1, L2, L3, Loss]\n",
    "\n",
    "    where L1, L2, L3, Loss are all Layer objects.\n",
    "\n",
    "    Each Layer object implements a function called 'forward'. forward simply\n",
    "    computes the output of a layer given its input. So instead of\n",
    "    Loss(L3(L2(L1(x))), we write\n",
    "    Loss.forward(L3.forward(L2.forward(L1.forward(x)))).\n",
    "    Doing this computation finishes the forward pass of backprop.\n",
    "\n",
    "    Each layer also implements a function called 'backward'. Backward is\n",
    "    responsible for the backward pass of backprop. After we have computed the\n",
    "    forward pass, we compute\n",
    "    L1.backward(L2.backward(L3.backward(Loss.backward(1))))\n",
    "    We give 1 as the input to Loss.backward because backward is implementing\n",
    "    the chain rule - it multiplies gradients together and so giving 1 as an\n",
    "    input makes the multiplication an identity operation.\n",
    "\n",
    "    The outputs of backward are a little subtle. Some layers may have a\n",
    "    parameter that specifies the function being computed by the layer. For\n",
    "    example, a Linear layer maintains a weight matrix, so that\n",
    "    Linear(x) = xW\n",
    "    for some matrix W.\n",
    "    The input to backward should be the gradient of the final loss with respect\n",
    "    to the output of the current layer. The output of backprop should be the\n",
    "    gradient of the final loss with respect to the input of the current layer,\n",
    "    which is just the output of the previous layer. This is why it is correct\n",
    "    to chain the outputs of backprop together. However, backward should ALSO\n",
    "    compute the gradient of the loss with respect to the current layer's\n",
    "    parameter and store this internally to be used in training.\n",
    "    '''\n",
    "    def __init__(self, parameter=None, name=None):\n",
    "        self.name = name\n",
    "        self.forward_called = False\n",
    "        self.parameter = parameter\n",
    "        self.grad = None\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''forward pass. Should compute layer and save relevant state\n",
    "        needed for backward pass.\n",
    "        Args:\n",
    "            input: input to this layer.\n",
    "        returns output of operation.\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, downstream_grad):\n",
    "        '''Performs backward pass.\n",
    "\n",
    "        This function should also set self.grad to be the gradient of the final\n",
    "        output of the computation (loss) with respect to the layer's parameters.\n",
    "\n",
    "        Args:\n",
    "            downstream_grad: gradient from downstream operation in the\n",
    "                computation graph. This package will only consider\n",
    "                computation graphs that result in scalar outputs at the final\n",
    "                node (e.g. loss function computations). As a result,\n",
    "                the dimension of downstream_grad should match the dimension of\n",
    "                the output of this layer.\n",
    "\n",
    "                Formally, if this operation computes F(x), and the final\n",
    "                computation computes a scalar, G(F(x)), then input_grad is\n",
    "                dG/dF.\n",
    "        returns:\n",
    "            gradient to pass to upstream layers. If the layer computes F(x, w),\n",
    "            where x is the input and w is the parameter of the layer, then\n",
    "            the return value should be dF(x,w)/dx * downstream_grad. Here,\n",
    "            x is in R^n, F(x, w) is in R^m, dF(x, w)/dx is a matrix in R^(n x m)\n",
    "            downstream_grad is in R^m and * indicates matrix multiplication.\n",
    "\n",
    "        We should also compute the gradient with respect to the parameter w.\n",
    "        Again by chain rule, this is dF(x, w)/dw * downstream_grad\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKZ3SP2lTpSJ"
   },
   "source": [
    "Below shows an example of the full implementation of the Bias layer, including the forward and backward function. Notice self.grad stores the gradient of the loss with respect to the current layer's parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JF30rs0ITosh"
   },
   "outputs": [],
   "source": [
    "class Bias(Layer):\n",
    "    '''adds a constant bias.'''\n",
    "\n",
    "    def __init__(self, bias, name=\"bias\"):\n",
    "        super(Bias, self).__init__(np.squeeze(bias), name)\n",
    "        self.weights = np.squeeze(bias)\n",
    "\n",
    "    def forward(self, input):  \n",
    "        self.input = input\n",
    "        return self.parameter + self.input\n",
    "\n",
    "    def backward(self, downstream_grad): \n",
    "        self.grad = np.sum(downstream_grad, tuple(range(downstream_grad.ndim - self.parameter.ndim)))\n",
    "        return downstream_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntLOrh72mHOW"
   },
   "source": [
    "## **Q2.1** Multiplication layers.\n",
    "\n",
    "Let's start with the basic linear and bias layer. Show the derivatives of linear and bias layer with respect to $X$ respectively.\n",
    "\n",
    "$Z_{linear} = WX$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhpqN_9O2fmz"
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "$\\frac{\\partial Z_{linear}}{\\partial X} = W $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gyrJM3O2k2U"
   },
   "source": [
    "Complete the forward and backward function of the linear layer. In backward, you should ALSO set the self.grad to be the gradient of the loss with respect to the current layer's parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4L8Dcfq-lTRd"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    '''Linear layer. Parameter is NxM matrix L, input is matrix v of size B x N\n",
    "    where B is batch size, output is vL.'''\n",
    "\n",
    "    def __init__(self, weights, name=\"Linear\"):\n",
    "        super(Linear, self).__init__(weights, name)\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, input):\n",
    "        ## -- ! code required  \n",
    "        self.input = input  \n",
    "        return np.dot(input, self.parameter)\n",
    "\n",
    "    def backward(self, downstream_grad):\n",
    "        '''downstream_grad should be NxB.'''\n",
    "        if len(downstream_grad.shape) != 2:\n",
    "            downstream_grad = np.reshape(\n",
    "                downstream_grad, (len(downstream_grad), 1))\n",
    "        self.grad = np.dot(self.input.T, downstream_grad)\n",
    "        return np.dot(downstream_grad, self.parameter.transpose())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icklJkILnhNB"
   },
   "source": [
    "## **Q2.2** Activation layers.\n",
    "\n",
    "Now let's look at the activation layers. Show the derivatives of ReLU and sigmoid. \n",
    "<p>\n",
    "$ReLU(x) = max(0,x)$\n",
    "</p> \n",
    "<p>\n",
    "$\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
    "</p> \n",
    "\n",
    "Hint: Let's assume the gradient of ReLU is 0 when x is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq5dtn6k49SR"
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "<p>$ \\frac{\\partial ReLU}{\\partial x} =\n",
    "\\begin{cases} \n",
    "0 ; \\text{if  }  x <= 0 \\\\\n",
    "1 ; \\text{if  }  x > 0 \\\\\n",
    "\\end{cases}\n",
    "$</p>\n",
    "\n",
    "$\\frac{\\partial \\sigma}{\\partial x} = \\frac{e^{-x}}{(1+e^{-x})^2} = \\frac{1}{1+e^{-x}}\\cdot(1-\\frac{1}{1+e^{-x}}) = \\sigma(x)\\cdot (1-\\sigma(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZPP7OTt49NQ"
   },
   "source": [
    "Complete the forward and backward functions. There is no need to update self.grad since there is no parameter in activation layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FgbalOrBlYkS"
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    '''ReLU layer. No parameters.'''\n",
    "\n",
    "    def __init__(self, name=\"ReLU\"):\n",
    "        super(ReLU, self).__init__(name=name)\n",
    "\n",
    "    def forward(self, input):\n",
    "        ## -- ! code required  \n",
    "        self.non_negative = input > 0\n",
    "        return np.maximum(input, 0.0)\n",
    "\n",
    "    def backward(self, downstream_grad):\n",
    "        ## -- ! code required  \n",
    "        return self.non_negative * downstream_grad\n",
    "\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    '''Sigmoid layer. No parameters.'''\n",
    "\n",
    "    def __init__(self, name=\"Sigmoid\"):\n",
    "        super(Sigmoid, self).__init__(name=name)\n",
    "\n",
    "    def forward(self, input):\n",
    "        ## -- ! code required  \n",
    "        self.output = np.exp(input) / (1.0 + np.exp(input))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, downstream_grad):\n",
    "        ## -- ! code required  \n",
    "        return (self.output - self.output**2) * downstream_grad \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU1LRCr_oI5o"
   },
   "source": [
    "## **Q2.3**  Loss *layers*.\n",
    "Define the mean square error as follows: \n",
    "<p>\n",
    "$MSE(\\hat y) = \\frac{1}{2N}\\sum_{i=1}^N(y_i - \\hat y_i)^2$. \n",
    "</p> \n",
    "where $y$ is the label and $\\hat y$ is your prediction. Show the gradient of MSE w.r.t $\\hat y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VghiDCb45L4M"
   },
   "source": [
    "**Solution** \n",
    "\n",
    "<p>\n",
    "$\\frac{\\partial MSE(\\hat y)}{\\partial \\hat y} = -\\frac{1}{N}(y -\\hat y)$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyxKVXh76VIi"
   },
   "source": [
    "Complete the forward and backward functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "47ZAc31L1squ"
   },
   "outputs": [],
   "source": [
    "class MeanSquaredError(Layer):\n",
    "    '''cross entropy loss.'''\n",
    "\n",
    "    def __init__(self, labels, name=\"Mean Squared Error\"):\n",
    "        super(MeanSquaredError, self).__init__(name=\"Mean Squared Error\")\n",
    "        self.labels = labels\n",
    "    def forward(self, input):\n",
    "        '''input is BxN, output is B'''\n",
    "        ## -- ! code required  \n",
    "        self.input = input\n",
    "        return np.dot((self.labels - self.input).T, (self.labels - self.input))/(2*self.input.shape[0])\n",
    "\n",
    "    def backward(self, downstream_grad):\n",
    "        ## -- ! code required  \n",
    "        grad = -(self.labels -self.input)/self.input.shape[0]\n",
    "        return grad * downstream_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sa8Lz_8Ro5F-"
   },
   "source": [
    "## **Q2.4** \n",
    "\n",
    "Now let's build a simple model using your layers, and compare the autograd results with the numeric derivatives. If everything is implemented in the correct way, the autograd results should be very closed to numeric grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bcJ46cGlh3c",
    "outputId": "32639536-dfc8-4774-ab9f-959cc81dc23a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking good!\n"
     ]
    }
   ],
   "source": [
    "# This function computes the derivative numerically using the formula (f(x+delta) - f(x))/delta with small delta. First, we would compute \n",
    "# f(x) which is the original output. Then we perturb the input by a small delta then compute f(x+delta). Finally, we calculate\n",
    "# the difference and divide by delta to get the derivative.\n",
    "def numerical_derivative(layers, input):\n",
    "    base_output = forward_layers(layers, input)\n",
    "    delta = 1e-7\n",
    "    \n",
    "    for layer in layers:\n",
    "        if layer.parameter is None:\n",
    "            continue\n",
    "        size = layer.parameter.size # total number of params\n",
    "        shape = layer.parameter.shape # shape of params\n",
    "        base_param = np.copy(layer.parameter)\n",
    "        perturb = np.zeros(size)\n",
    "        grad = np.zeros(size)\n",
    "         \n",
    "        for i in range(size):\n",
    "            perturb[i] = delta # only current i-th perturb is non-zero\n",
    "            layer.parameter = base_param + np.reshape(perturb, shape) # make a small change (delta) on the i-th parameter\n",
    "            perturb_output = forward_layers(layers, input) # new output after adding a small change (delta) on the i-th parameter\n",
    "            grad[i] = (perturb_output - base_output) / delta # update the grad of i-th parameter\n",
    "            perturb[i] = 0.0 # set it back to zero\n",
    "            \n",
    "        layer.parameter = base_param\n",
    "        layer.grad = np.reshape(np.copy(grad), shape)\n",
    "\n",
    "def forward_layers(layers, input):\n",
    "    '''Forward pass on all the layers. Must be called before backwards pass.'''\n",
    "    output = input\n",
    "    for layer in layers:\n",
    "        output = layer.forward(output)\n",
    "    #assert output.size == 1, \"only supports computations that output a scalar!\"\n",
    "    return output\n",
    "\n",
    "\n",
    "def backward_layers(layers):\n",
    "    '''runs a backward pass on all the layers.\n",
    "    after this function is finished, look at layer.grad to find the\n",
    "    gradient with respect to that layer's parameter.'''\n",
    "    downstream_grad = np.array([1])\n",
    "    for layer in reversed(layers):\n",
    "        downstream_grad = layer.backward(downstream_grad)\n",
    "\n",
    "\n",
    "def zero_grad(layers):\n",
    "    for layer in layers:\n",
    "        layer.zero_grad()\n",
    "\n",
    "        \n",
    "def test_autograd():\n",
    "    h = 2\n",
    "    b = 3\n",
    "    input = np.random.normal(np.zeros((b, h)))\n",
    "    labels = np.array([0,0,1]).reshape(3,1)\n",
    "    layers = [\n",
    "        Linear(np.random.normal(size=(h, 2 * h))),\n",
    "        Sigmoid(),\n",
    "        Bias(np.array([np.random.normal()])),\n",
    "        Linear(np.random.normal(size=(2 * h, 3 * h))),\n",
    "        ReLU(),\n",
    "        Linear(np.random.normal(size=(3 * h, 1))),\n",
    "        MeanSquaredError(labels)\n",
    "    ]\n",
    "    output = forward_layers(layers, input)\n",
    "    backward_layers(layers)\n",
    "    analytics = [np.copy(layer.grad)\n",
    "                 for layer in layers if layer.grad is not None]\n",
    "    zero_grad(layers)\n",
    "\n",
    "    numerical_derivative(layers, input)\n",
    "    numerics = [np.copy(layer.grad)\n",
    "                for layer in layers if layer.grad is not None]  \n",
    "    # Computing the difference between the derivative of our implemented function and the numerical derivative \n",
    "    diff = np.sum([np.linalg.norm(analytic - numeric)/np.linalg.norm(numeric)\n",
    "                   for analytic, numeric in zip(analytics, numerics)])\n",
    "    \n",
    "    assert diff < 1e-5, \"autograd differs by {} from numeric grad!\".format(diff)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_autograd()\n",
    "    print(\"looking good!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y613iZJ0pjO2"
   },
   "source": [
    "## Problem 3: Implementing a simple MLP.\n",
    "\n",
    "In this problem we will develop a neural network with fully-connected layers, aka Multi-Layer Perceptron (MLP) using the layers from Problem 2. Below, we initialize toy data  that we will use to develop your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gD5cHE09pVAr",
    "outputId": "154b8f61-9c26-4040-d9f0-ca0ac8754428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  (100, 1)\n",
      "y =  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create some toy data\n",
    "X = np.linspace(-1, 1, 100).reshape(-1,1)\n",
    "y = 5*X + 2 + 0.5*np.random.normal()\n",
    "\n",
    "print ('X = ', X.shape)\n",
    "print('y = ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtBLq2v7EiUK"
   },
   "source": [
    "We will use the following class `TwoLayerMLP` to implement our network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HM1ia9y0RlUF"
   },
   "outputs": [],
   "source": [
    "class TwoLayerMLP(object):\n",
    "    def __init__(self, input_size, hidden_size, label_size, std=1e-1, activation='sigmoid'):\n",
    "        np.random.seed(0)\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.params = {}\n",
    "\n",
    "        ## TODO: Initialize your parameters below using input_size, hidden_size, label_size\n",
    "        ## the weights of the linear layers are normally distributed with standard deviation = std\n",
    "        ## and mean = 0. The bias is zero\n",
    "        \n",
    "        self.activation = 'sigmoid' \n",
    "        self.params['W1'] = std * np.random.normal(size=(input_size, hidden_size))\n",
    "        self.params['W2'] = std * np.random.normal(size=(hidden_size, label_size))\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['b2']  = np.zeros(label_size)\n",
    "        self.activation = 'sigmoid' \n",
    "\n",
    "        self.models = [\n",
    "                  Linear(self.params['W1']),\n",
    "                  Bias(self.params['b1']),\n",
    "                  Sigmoid(),\n",
    "                  Linear(self.params['W2']),\n",
    "                  Bias(self.params['b2'])\n",
    "                ]    \n",
    "           \n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        _, C = W2.shape\n",
    "        N, D = X.shape\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Finish the forward pass, and compute the loss. This should be the \n",
    "        # data loss. Store the result in the variable loss, which should be a scalar. \n",
    "        # Use the CrossEntropy loss. So that your results match ours.\n",
    "        ###########################################################################  \n",
    "        ## -- ! code required  \n",
    "      \n",
    "        scores = forward_layers(self.models, X)  \n",
    "        loss_layer  = MeanSquaredError(y)   \n",
    "        loss = loss_layer.forward(scores)        \n",
    "        ###########################################################################\n",
    "        #                            END OF YOUR CODE\n",
    "        ###########################################################################\n",
    "\n",
    "\n",
    "        grads = {}\n",
    "        ###########################################################################\n",
    "        # TODO: Compute the backward pass, computing the derivatives of the weights\n",
    "        # and biases. Store the results in the grads dictionary. For example,\n",
    "        # grads['W1'] should store the gradient on W1, and be a matrix of same size\n",
    "        ###########################################################################\n",
    "        ## -- ! code required  \n",
    "        self.backward_layers(loss_layer.backward(np.array([1])))\n",
    "        dW1 = self.models[0].grad\n",
    "        db1 = self.models[1].grad\n",
    "        dW2 = self.models[3].grad\n",
    "        db2 = self.models[4].grad    \n",
    "        grads['W2'] = dW2\n",
    "        grads['b2'] = db2\n",
    "        grads['W1'] = dW1\n",
    "        grads['b1'] = db1\n",
    "        ###########################################################################\n",
    "        #                            END OF YOUR CODE\n",
    "        ###########################################################################\n",
    "        return loss, grads\n",
    "\n",
    "    def backward_layers(self, downstream_grad):\n",
    "        '''runs a backward pass on all the layers.\n",
    "        after this function is finished, look at layer.grad to find the\n",
    "        gradient with respect to that layer's parameter.'''\n",
    "        for layer in reversed(self.models):\n",
    "            downstream_grad = layer.backward(downstream_grad)\n",
    "\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            reg=1e-5, num_epochs=10,\n",
    "            batch_size=1, verbose=False):\n",
    "\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = 1 #int(max(num_train / batch_size, 1))\n",
    "        epoch_num = 0\n",
    "\n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        grad_magnitude_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        np.random.seed(1)\n",
    "        for epoch in range(num_epochs):\n",
    "            # fixed permutation (within this epoch) of training data\n",
    "            perm = np.random.permutation(num_train)\n",
    "\n",
    "            # go through minibatches\n",
    "            for it in range(iterations_per_epoch):\n",
    "                X_batch = None\n",
    "                y_batch = None\n",
    "\n",
    "                # Create a random minibatch\n",
    "                idx = perm[it*batch_size:(it+1)*batch_size]\n",
    "                X_batch = X[idx, :]\n",
    "                y_batch = y[idx]\n",
    "                # Compute loss and gradients using the current minibatch\n",
    "                loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "                #print(\"loss\", loss)\n",
    "                loss_history.append(loss)\n",
    "\n",
    "                # do gradient descent\n",
    "                for param in self.params:\n",
    "                    self.params[param] -= grads[param] * learning_rate\n",
    "\n",
    "                # record gradient magnitude (Frobenius) for W1\n",
    "                grad_magnitude_history.append(np.linalg.norm(grads['W1']))\n",
    "\n",
    "            # Decay learning rate\n",
    "            learning_rate *= learning_rate_decay\n",
    "\n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'grad_magnitude_history': grad_magnitude_history, \n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSkRivC0soqb"
   },
   "source": [
    "### Q3.1 Forward pass\n",
    "\n",
    "Our 2-layer MLP uses a mean squared error loss layer defined in Problem 2.\n",
    "\n",
    "Please take a look at method `TwoLayerMLP.loss`. This function takes in the data and weight parameters, and computes the class scores (output of the forward layer), the loss ($L$), and the gradients on the parameters. \n",
    "\n",
    "- Use the layers designed in **Problem 2** and implement the first part of the function to compute `scores` and `loss`. Afterwards, run the following two test cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGCgTsMXDuB-",
    "outputId": "3809d00a-61b8-4c74-aefc-919fff951b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Your scores:\n",
      "\n",
      "2.0038506582894944\n",
      "\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "6.582894944706652e-07\n",
      "\n",
      "\n",
      "(2) Your loss: 6.177027\n",
      "Difference between your loss and correct loss:\n",
      "1.177026793405319\n"
     ]
    }
   ],
   "source": [
    "input_size = 1\n",
    "hidden_size = 10\n",
    "num_classes = 1\n",
    "\n",
    "net = TwoLayerMLP(input_size, hidden_size, num_classes)\n",
    "scores = forward_layers(net.models, X)\n",
    "print ('(1) Your scores:\\n')\n",
    "print (np.linalg.norm(scores))\n",
    "print ('\\n')\n",
    "correct_norm = 2.00385\n",
    "# # The difference should be very small (< 1e-4)\n",
    "print ('Difference between your scores and correct scores:')\n",
    "print (np.sum(np.abs(np.linalg.norm(scores) -correct_norm)))\n",
    "print ('\\n')\n",
    "\n",
    "loss, _ = net.loss(X, y, reg=0.1)\n",
    "correct_loss = 5\n",
    "\n",
    "# Since we generate random data your loss would not be the same as the correct loss.\n",
    "# However, the difference should fairly small (less than 1 or 2)\n",
    "print ('(2) Your loss: %f'%(loss))\n",
    "print ('Difference between your loss and correct loss:')\n",
    "print (np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9cchDnkEX6I"
   },
   "source": [
    "## **Q3.2** Backward pass\n",
    "- Implement the second part to compute gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`, stored in `grads`. \n",
    "\n",
    "Hint: you can quickly get the gradients with respect to parameters by calling **self.backward_layers**(downstream_grad).\n",
    "\n",
    "Now debug your backward pass using a numeric gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4QOW0VpGohH",
    "outputId": "bf828835-fe9a-4b3a-c963-20c6629c78c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 max relative error: 4.613902e-11\n",
      "b2 max relative error: 2.281743e-11\n",
      "W1 max relative error: 2.975745e-09\n",
      "b1 max relative error: 5.500333e-09\n"
     ]
    }
   ],
   "source": [
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "\n",
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "  \"\"\" \n",
    "  a naive implementation of numerical gradient of f at x \n",
    "  - f should be a function that takes a single argument\n",
    "  - x is the point (numpy array) to evaluate the gradient at\n",
    "  \"\"\" \n",
    "\n",
    "  fx = f(x) # evaluate function value at original point\n",
    "  grad = np.zeros_like(x)\n",
    "  # iterate over all indexes in x\n",
    "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "\n",
    "    # evaluate function at x+h\n",
    "    ix = it.multi_index\n",
    "    oldval = x[ix]\n",
    "    x[ix] = oldval + h # increment by h\n",
    "    fxph = f(x) # evalute f(x + h)\n",
    "    x[ix] = oldval - h\n",
    "    fxmh = f(x) # evaluate f(x - h)\n",
    "    x[ix] = oldval # restore\n",
    "\n",
    "    # compute the partial derivative with centered formula\n",
    "    grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "    if verbose:\n",
    "      print (ix, grad[ix])\n",
    "    it.iternext() # step to next dimension\n",
    "\n",
    "  return grad\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.1)\n",
    "\n",
    "# these should all be very small\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.1)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print ('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4So3UarqGLbl"
   },
   "source": [
    "## **Q3.3** Train the Sigmoid network\n",
    "To train the network we will use stochastic gradient descent (SGD), implemented in `TwoLayerNet.train`. Train the two-layer network and plot the ['loss_history']. We don't expect you to optimize the training process. As long as the the loss graph looks resonable (loss is going down), you will get full credits. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "dtnITSRP4Z9k",
    "outputId": "fff7c097-d4df-48db-8435-ad8a64438132"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb8d4295048>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = net.train(X, y, X, y,learning_rate=1, reg=1e-5, batch_size = 1, num_epochs=200, verbose=False)\n",
    "plt.plot(np.array(stats['loss_history']).reshape(-1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyQVjefVIwcQ"
   },
   "source": [
    "# Problem 4: Pytorch Intro\n",
    "## **Q4.0**: Pytorch tutorials\n",
    "This homework will introduce you to [PyTorch](https://pytorch.org), currently the fastest growing deep learning library, and the one we will use in this course.\n",
    "\n",
    "Before starting the homework, please go over these introductory tutorials on the PyTorch webpage:\n",
    "\n",
    "*   [60-minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "p9GI4YuDL-8D"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxw9G9oyI67G"
   },
   "source": [
    "The `torch.Tensor` class is the basic building block in PyTorch and is used to hold data and parameters. The `autograd` package provides automatic differentiation for all operations on Tensors. After reading about Autograd in the tutorials above,  we will implement a few simple examples of what Autograd can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9kS5w0m4pAv"
   },
   "source": [
    "## **Q4.1**. Simple function\n",
    " Use `autograd` to do backpropagation on the simple function we saw in lecture, $f=(x+y)*z$. \n",
    "\n",
    "**Q4.1.1** Create the three inputs with values $x=-2$, $y=5$ and $z=-4$ as tensors and set `requires_grad=True` to track computation on them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RcrfpCMK4rzh",
    "outputId": "645c2728-ea9e-412b-9420-fc86ffc5242b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.], requires_grad=True) tensor([5.], requires_grad=True) tensor([-4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "x = torch.tensor([float(-2)], requires_grad=True)\n",
    "y = torch.tensor([float(5)], requires_grad=True)\n",
    "z = torch.tensor([float(-4)], requires_grad=True)\n",
    "print(x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiEkkQJH4tah"
   },
   "source": [
    "**Q4.1.2** Compute the $q=x+y$ and $f=q \\times z$ functions, creating tensors for them in the process. Print out $q,f$, then run `f.backward(retain_graph=True)`, to compute the gradients w.r.t. $x,y,z$. The `retain_graph` attribute tells autograd to keep the computation graph around after backward pass as opposed deleting it (freeing some memory). Print the gradients. Note that the gradient for $q$ will be `None` since it is an intermediate node, even though `requires_grad` for it is automatically set to `True`. To access gradients for intermediate nodes in PyTorch you can use hooks as mentioned in [this answer](https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94/2). Compute the values by hand (or check the slides) to verify your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ps9rbudc4vkY",
    "outputId": "f92c80c7-4f4b-4e36-8021-d099b03f5bb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], grad_fn=<AddBackward0>) tensor([-12.], grad_fn=<MulBackward0>)\n",
      "tensor([-4.]) tensor([-4.]) tensor([3.]) None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /Users/distiller/project/pytorch/build/aten/src/ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "q = x + y\n",
    "f = q*z\n",
    "print(q, f)\n",
    "f.backward(retain_graph=True)\n",
    "print(x.grad, y.grad, z.grad, q.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUFLoE6d4xQU"
   },
   "source": [
    "**Q4.1.3** If we now run `backward()` again, it will add the gradients to their previous values. Try it by running the above cell multiple times. This is useful in some cases, but if we just wanted to re-compute the gradients again, we need to zero them first, then run `backward()`. Add this step, then try running the  backward function multiple times to make sure the answer is the same each time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mbT5xYs7vCh8",
    "outputId": "0bc95619-9e26-40f5-a7c0-dd33c39ceb04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.]) tensor([0.]) tensor([0.]) None\n",
      "tensor([-4.]) tensor([-4.]) tensor([3.]) None\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "z.grad.zero_()\n",
    "print(x.grad, y.grad, z.grad, q.grad)\n",
    "f.backward(retain_graph=True)\n",
    "print(x.grad, y.grad, z.grad, q.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXD55v9vMkC"
   },
   "source": [
    "## **Q4.2** Neuron\n",
    " Implement the function corresponding to one neuron (logistic regression unit) that we saw in the lecture and compute the gradient w.r.t. $x$ and $w$. The function is $f=\\sigma(w^Tx)$ where $\\sigma()$ is the sigmoid function. Initialize $x=[-1, -2, 1]$ and the weights to $w=[2, -3, -3]$ where $w_3$ is the bias. Print out the gradients and double check their values by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgLLALfwvO4r",
    "outputId": "60c0f030-6628-4020-c83c-59c77022601f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x= tensor([-1., -2.,  1.], requires_grad=True) \n",
      "w= tensor([ 2., -3., -3.], requires_grad=True) \n",
      "f(x,w)= tensor(0.7311, grad_fn=<MulBackward0>)\n",
      "The gradient of f() w.r.t. x is tensor([ 0.3932, -0.5898, -0.5898])\n",
      "The gradient of f() w.r.t. w is tensor([-0.1966, -0.3932,  0.1966])\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "x = torch.tensor([-1., -2., 1.], requires_grad=True)\n",
    "w = torch.tensor([2., -3., -3.], requires_grad=True)\n",
    "f1 = torch.dot(x, w)\n",
    "f2 = -1 * f1\n",
    "f3 = torch.exp(f2)\n",
    "f = 1 / (1 + f3)\n",
    "print(\"\\nx=\", x, \"\\nw=\", w, \"\\nf(x,w)=\", f)\n",
    "\n",
    "f.backward()\n",
    "print(\"The gradient of f() w.r.t. x is\", x.grad)\n",
    "print(\"The gradient of f() w.r.t. w is\", w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTve24kfvRVG"
   },
   "source": [
    "## **Q4.3**. torch.nn\n",
    " We will now implement the same neuron function $f$ with the same variable values as in Q1.2, but using the `Linear` class from `torch.nn`, followed by the [Sigmoid](https://pytorch.org/docs/stable/nn.html#torch.nn.Sigmoid) class. In general, many useful functions are already implemented for us in this package. Compute the gradients $\\partial f/\\partial w$ by running `backward()` and print them out (they will be stored in the Linear variable, e.g. in `.weight.grad`.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGZ16CQFvTa-",
    "outputId": "1e28d8c9-07c0-4f31-f2d9-5e1b7f4fddfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights: Parameter containing:\n",
      "tensor([[ 2., -3.]], requires_grad=True)\n",
      "\n",
      "bias: Parameter containing:\n",
      "tensor([-3.], requires_grad=True)\n",
      "\n",
      "f: tensor([0.7311], grad_fn=<SigmoidBackward0>)\n",
      "The gradient of f() w.r.t. w is tensor([[-0.1966, -0.3932]]) tensor([0.1966])\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "import torch.nn as nn\n",
    "\n",
    "linear_f = nn.Linear(2, 1)\n",
    "linear_f.weight.data = torch.tensor([[ 2., -3.]]);\n",
    "linear_f.bias.data = torch.tensor([ -3.]);\n",
    "\n",
    "print(\"\\nweights:\", linear_f.weight)\n",
    "print(\"\\nbias:\",linear_f.bias)\n",
    "\n",
    "sigmoid_f = nn.Sigmoid()\n",
    "\n",
    "input = torch.tensor([-1., -2.])\n",
    "\n",
    "f = sigmoid_f(linear_f(input))\n",
    "print(\"\\nf:\", f)\n",
    "\n",
    "# do backprop\n",
    "f.backward()\n",
    "\n",
    "print(\"The gradient of f() w.r.t. w is\", linear_f.weight.grad, linear_f.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fj_uO0_dvVk5"
   },
   "source": [
    "## **Q4.4** Module\n",
    " Now lets put these two functions (Linear and Sigmoid) together into a \"module\". Read the [Neural Networks tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) if you have not already.\n",
    "\n",
    "**Q4.4.1** Make a subclass of the `Module` class, called `Neuron`. Set variables to the same values as above. You will need to define the `__init__` and `forward`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rcTn1__FvXVy"
   },
   "outputs": [],
   "source": [
    "# solution here\n",
    "import torch.nn as nn\n",
    "\n",
    "class Neuron(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Neuron, self).__init__()\n",
    "        # an affine operation: y = weight*x + bias, with fixed parameters\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.linear.weight.data = torch.tensor([[ 2., -3.]]);\n",
    "        self.linear.bias.data = torch.tensor([-3.]);\n",
    "        # a sigmoid function, elementwise\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bum-YdWsvZ_k"
   },
   "source": [
    "**Q4.4.2** Now create a  variable of your `Neuron` class called `my_neuron` and run backpropagation on it. Print out the gradients again. Make sure you zero out the gradients first, by calling `.zero_grad()` function of the parent class. Even if you will not re-compute the backprop, it is good practice to do this every time to avoid accumulating gradient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZRuiOggvbud",
    "outputId": "b95d146e-7afc-481e-d5e3-15b835e2637b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "The weights are: Parameter containing:\n",
      "tensor([[ 2., -3.]], requires_grad=True)\n",
      "\n",
      "f(x,w)= tensor([0.7311], grad_fn=<SigmoidBackward0>)\n",
      "The gradient of f() w.r.t. w is tensor([[-0.1966, -0.3932]]) tensor([0.1966])\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "my_neuron = Neuron()\n",
    "print(my_neuron)\n",
    "params = list(my_neuron.parameters())\n",
    "\n",
    "print(\"The weights are:\", params[0])  # linear layer's .weight\n",
    "input = torch.tensor([-1., -2.])\n",
    "out = my_neuron(input)\n",
    "print(\"\\nf(x,w)=\", out)\n",
    "my_neuron.zero_grad()\n",
    "out.backward()\n",
    "print(\"The gradient of f() w.r.t. w is\", params[0].grad, params[1].grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7Qm62nKvhJZ"
   },
   "source": [
    "## **Q4.5**. Loss and SGD\n",
    " Now, lets train our neuron on some data. The code below creates a toy dataset containing a few inputs $x$ and outputs $y$ (a binary 0/1 label), as well as a function that plots the data and current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "xq3jAoO8vilS",
    "outputId": "1af4175b-6aed-44ef-9a38-d471d96ea22d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 = 2.0 w1 = -3.0 bias = -3.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create some toy 2-D datapoints with binary (0/1) labels\n",
    "x = torch.tensor([[1.2, 1], [0.2, 1.4], [0.5, 0.5], \n",
    "                  [-1.5, -1.3], [0.2, -1.4], [-0.7, -0.5]])\n",
    "y = torch.tensor([0, 0, 0, 1, 1, 1 ])\n",
    "\n",
    "def plot_soln(x, y, params):\n",
    "  plt.plot(x[y==1,0], x[y==1,1], 'r+')\n",
    "  plt.plot(x[y==0,0], x[y==0,1], 'b.')\n",
    "  plt.grid(True)\n",
    "  plt.axis([-2, 2, -2, 2])\n",
    "  \n",
    "  # NOTE : This may depend on how you implement Neuron.\n",
    "  #   Change accordingly\n",
    "  w0 = params[0][0][0].item()\n",
    "  w1 = params[0][0][1].item()\n",
    "  bias = params[1][0].item()\n",
    "  \n",
    "  print(\"w0 =\", w0, \"w1 =\", w1, \"bias =\", bias)\n",
    "  dbx = torch.tensor([-2, 2])\n",
    "  dby = -(1/w1)*(w0*dbx + bias)  # plot the line corresponding to the weights and bias\n",
    "  plt.plot(dbx, dby)\n",
    "\n",
    "params = list(my_neuron.parameters())\n",
    "plot_soln(x, y, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOYoZkq1vkxt"
   },
   "source": [
    "**Q4.5.1** Declare an object `criterion` of type `nn.CrossEntropyLoss`. Note that this can be called as a function on two tensors, one representing the network outputs and the other, the targets that the network is being trained to predict, to return the loss. Print the value of the loss on the dataset using the initial weights and bias defined above in Q1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpxpG8Uovqzv",
    "outputId": "c929cee9-17b0-4e5e-d7cf-9bac6eac6a61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.942585289478302\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# forward + backward + optimize\n",
    "outputs = my_neuron(x)\n",
    "loss = criterion(torch.cat((outputs, 1-outputs), axis=1), y)\n",
    "print(\"loss =\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJFVZfscvtix"
   },
   "source": [
    "**Q4.5.2** Print out the chain of `grad_fn` functions backwards starting from `loss.grad_fn`  to demonstrate what backpropagation will be run on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fIKTYZMvvhy",
    "outputId": "e093bfdf-c781-43b2-f12d-bc8f668e4b0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NllLossBackward0 object at 0x7fb8d5ad9be0>\n",
      "<LogSoftmaxBackward0 object at 0x7fb8d5ad9f60>\n",
      "<CatBackward0 object at 0x7fb8d5ad9be0>\n",
      "<SigmoidBackward0 object at 0x7fb8d5ad97f0>\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "print(loss.grad_fn)  \n",
    "print(loss.grad_fn.next_functions[0][0])  \n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  \n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdX99YRGvxiU"
   },
   "source": [
    "**Q4.5.3** Run the Stochastic Gradient Descent (SGD) optimizer from the `torch.optim` package to train your classifier on the toy dataset. Use the entire dataset in each batch. Use a learning rate of $0.01$ (no other hyperparameters). You will need to write a training loop that uses the `.step()` function of the optimizer. Plot the solution and print the loss after 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "U2L6QMsSvzrt",
    "outputId": "2bd2f12f-53ab-4002-b1a3-4dc2a71b8079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.3273821771144867\n",
      "w0 = 4.141496658325195 w1 = 2.4262382984161377 bias = -0.8129552006721497\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# solution here\n",
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(my_neuron.parameters(), lr=0.01)\n",
    "\n",
    "# training loop\n",
    "for i in range(10000):\n",
    "  # in your training loop:\n",
    "  optimizer.zero_grad()   # zero the gradient buffers\n",
    "  outputs = my_neuron(x)\n",
    "  loss = criterion(torch.cat((outputs, 1-outputs), axis=1), y)\n",
    "  #print(\"loss =\", loss.item())\n",
    "  loss.backward()\n",
    "  optimizer.step()    # Does the update\n",
    "\n",
    "print(\"loss =\", loss.item())\n",
    "params = list(my_neuron.parameters())\n",
    "plot_soln(x, y, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed2aFhiYK-vb"
   },
   "source": [
    "**Q4.5.4** How many thousands of iterations does it take (approximately) until the neuron learns to classify the data correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tuMoY6JLG_C"
   },
   "source": [
    "***Solution***\n",
    "\n",
    "Answer: approximately 5,000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MdGRt1jOmBX"
   },
   "source": [
    "## **Q4.6**. Hidden space ablation\n",
    "\n",
    "Now let's look at the size of network's hidden space. We will create and train a **2-layer MLP** network on the [SVHN Dataset](http://ufldl.stanford.edu/housenumbers/).\n",
    "\n",
    "The SVHN dataset consists of photos of house numbers, collected automatically using Google's Street View. Recognizing multi-digit numbers in photographs captured at street level is an important component of modern-day map making. Googles Street View imagery contains hundreds of millions of geo-located 360 degree panoramic images. The ability to automatically transcribe an address number from a geo-located patch of pixels and associate the transcribed number with a known street address helps pinpoint, with a high degree of accuracy, the location of the building it represents. Below are example images from the dataset. Note that for this dataset, each image (32x32 pixels) has been cropped around a single number in its center, which is the number we want to classify.\n",
    "\n",
    "![SVHN images](http://ufldl.stanford.edu/housenumbers/32x32eg.png)\n",
    "\n",
    "In this problem, we turn the input images into grayscale and then flat them into 1-D vector. First, download the SVHN dataset using `torchvision` and display the images in the first batch. Take a look at the [Training a Classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) tutorial for an example. Follow the settings used there, such as the normalization, batch size of 4 for the `torch.utils.data.DataLoader`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262,
     "referenced_widgets": [
      "cccb954e47ca41e49684afec8de9fb57",
      "98c64c764d6a4fe59acf37dc5e7c533b",
      "b1c6810a66ca443983e516f0de343ef2",
      "8c399048ca1f408eadd061478a3176fd",
      "94628a6b6a054a7a8135f944fb1b249a",
      "656ac10bf2ef430aa238bb1e77cbf531",
      "21ab7cfba07547c78705b876219fce21",
      "51825b850d544e8e838c2a692e897b16",
      "06eee8b3e3a64d0faf51e9cc357b7d5b",
      "ab6129b6a29646e293209acd27155219",
      "c636f1a2174e47f78ad321c47d9e8262"
     ]
    },
    "id": "dtdl8h1eY2dZ",
    "outputId": "ee2f49aa-15a1-40d3-e5d8-537f7f9a48c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88.2%"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.SVHN(root='./data', split='train', transform=transform, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBDzvc_nLfGJ"
   },
   "source": [
    "### Q4.6.1 2-layer MLP \n",
    "\n",
    "Next, we will train a 2-layer MLP on the data. We have defined a simple 2-layer MLP for you with two fc layers and ReLU activation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWu68j0AaYD1"
   },
   "outputs": [],
   "source": [
    "# solution here\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Neuron(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Neuron, self).__init__()\n",
    "        self.l1 = nn.Linear(1024, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7OMjOH0MXpR"
   },
   "source": [
    "You can check the number of parameters in the model by printing out the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyODwJ0rJYq1",
    "outputId": "1a1999c6-56a9-4667-daf6-f0b97a280361"
   },
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "  print(\"model_summary\")\n",
    "  print()\n",
    "  print(\"Layer_name\"+\"\\t\"*7+\"Number of Parameters\")\n",
    "  print(\"=\"*100)\n",
    "  model_parameters = [layer for layer in model.parameters() if layer.requires_grad]\n",
    "  layer_name = [child for child in model.children()]\n",
    "  j = 0\n",
    "  total_params = 0\n",
    "  print(\"\\t\"*10)\n",
    "  for i in layer_name:\n",
    "    print()\n",
    "    param = 0\n",
    "    try:\n",
    "      bias = (i.bias is not None)\n",
    "    except:\n",
    "      bias = False  \n",
    "    if not bias:\n",
    "      param =model_parameters[j].numel()+model_parameters[j+1].numel()\n",
    "      j = j+2\n",
    "    else:\n",
    "      param =model_parameters[j].numel()\n",
    "      j = j+1\n",
    "    print(str(i)+\"\\t\"*3+str(param))\n",
    "    total_params+=param\n",
    "  print(\"=\"*100)\n",
    "  print(f\"Total Params:{total_params}\")       \n",
    "\n",
    "my_neuron = Neuron(10)\n",
    "model_summary(my_neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOqfDwW7MmCM"
   },
   "source": [
    "Instantiate the cross-entropy loss `criterion`, and an SGD optimizer from the `torch.optim` package with learning rate $.001$ and momentum $.9$. You may also want to enable GPU training using `torch.device()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSGrluSuM0Th",
    "outputId": "1e4f31cf-3e18-4f8e-bd6b-dc4a0f96b2f8"
   },
   "outputs": [],
   "source": [
    "# solution here\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(my_neuron.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if we set the hardware to GPU in the Notebook settings, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "my_neuron.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnYmjlVnM-a0"
   },
   "source": [
    "### Q4.6.2 Training\n",
    "Complete the training loop that makes five full passes through the dataset (five epochs) using SGD. Your batch size should be 4 and hidden size is 10. \n",
    "*italicized text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JcsoLpkcA0N",
    "outputId": "142583bd-1343-4dfb-bb0e-637f77be00cf"
   },
   "outputs": [],
   "source": [
    "my_neuron = Neuron(10)\n",
    "\n",
    "# create your optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(my_neuron.parameters(), lr=0.001, momentum=0.9)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# num of epoch\n",
    "stats = []\n",
    "for epoch in range(5):\n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    ## -- ! code required  \n",
    "    optimizer.zero_grad()   \n",
    "    outputs = my_neuron(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, running_loss / 2000))\n",
    "        stats.append(running_loss / 2000)\n",
    "        running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCnLhyMWNdU9"
   },
   "source": [
    "Train the model again but this time set the hidden size as 100. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "id": "yZ2AEnXYdVgu",
    "outputId": "2c935856-5ac1-4aeb-e9e7-7ffeb72fb975"
   },
   "outputs": [],
   "source": [
    "## -- ! code required  \n",
    "my_neuron_large = Neuron(100)\n",
    "\n",
    "# create your optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(my_neuron_large.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# num of epoch\n",
    "stats_v2 = []\n",
    "for epoch in range(5):\n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data\n",
    "    #inputs, labels = inputs.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    outputs = my_neuron_large(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, running_loss / 2000))\n",
    "        stats_v2.append(running_loss / 2000)\n",
    "        running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cGSgfUiNn63"
   },
   "source": [
    "### Q4.6.3\n",
    "\n",
    "Compare the performance and what can you learn from the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "t8aVy24ddj9B",
    "outputId": "c1e1e14d-e0b0-4b47-fb8e-20c0e3d23f50"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(stats))*2000,stats, label='hidden_size = 10')\n",
    "plt.plot(np.arange(len(stats_v2))*2000, stats_v2, label='hidden_size = 100')\n",
    "plt.xlabel('Num of steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXCDFdORONV2"
   },
   "source": [
    "***[ double click to enter solution ]***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VLcR3G_f-xC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysPlY-XIgLOj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Hw2_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06eee8b3e3a64d0faf51e9cc357b7d5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "21ab7cfba07547c78705b876219fce21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "51825b850d544e8e838c2a692e897b16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "656ac10bf2ef430aa238bb1e77cbf531": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c399048ca1f408eadd061478a3176fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab6129b6a29646e293209acd27155219",
      "placeholder": "",
      "style": "IPY_MODEL_c636f1a2174e47f78ad321c47d9e8262",
      "value": " 182041600/? [00:02&lt;00:00, 82484311.50it/s]"
     }
    },
    "94628a6b6a054a7a8135f944fb1b249a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98c64c764d6a4fe59acf37dc5e7c533b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_656ac10bf2ef430aa238bb1e77cbf531",
      "placeholder": "",
      "style": "IPY_MODEL_21ab7cfba07547c78705b876219fce21",
      "value": ""
     }
    },
    "ab6129b6a29646e293209acd27155219": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1c6810a66ca443983e516f0de343ef2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51825b850d544e8e838c2a692e897b16",
      "max": 182040794,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_06eee8b3e3a64d0faf51e9cc357b7d5b",
      "value": 182040794
     }
    },
    "c636f1a2174e47f78ad321c47d9e8262": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cccb954e47ca41e49684afec8de9fb57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_98c64c764d6a4fe59acf37dc5e7c533b",
       "IPY_MODEL_b1c6810a66ca443983e516f0de343ef2",
       "IPY_MODEL_8c399048ca1f408eadd061478a3176fd"
      ],
      "layout": "IPY_MODEL_94628a6b6a054a7a8135f944fb1b249a"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
